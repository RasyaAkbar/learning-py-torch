{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQUKSko8nSk29ZUc3Jri5p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "edd083b1715b4a3983a6f0518ba9dbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8855c4298fda405ba18f500c29213415",
              "IPY_MODEL_2024df471218498792b2663e764adce5",
              "IPY_MODEL_523c78657bd94722941e9c81f7f6cd28"
            ],
            "layout": "IPY_MODEL_6eac04bae33447b5a655964794d3e0d4"
          }
        },
        "8855c4298fda405ba18f500c29213415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64349dee113941e8bfd52e1117505506",
            "placeholder": "​",
            "style": "IPY_MODEL_4d28b08ab35d4418821a31ff496b2983",
            "value": "100%"
          }
        },
        "2024df471218498792b2663e764adce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fddb11daf2dc44f8b5ada8eb5246cf9b",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f83c77e8e92e4c4a97b5e89378d0e22f",
            "value": 5
          }
        },
        "523c78657bd94722941e9c81f7f6cd28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ca5ef4ed32f4a08b2a1390d607d8b3b",
            "placeholder": "​",
            "style": "IPY_MODEL_fed07ec0c049402997045b96001a1bbc",
            "value": " 5/5 [00:12&lt;00:00,  2.30s/it]"
          }
        },
        "6eac04bae33447b5a655964794d3e0d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64349dee113941e8bfd52e1117505506": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d28b08ab35d4418821a31ff496b2983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fddb11daf2dc44f8b5ada8eb5246cf9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83c77e8e92e4c4a97b5e89378d0e22f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ca5ef4ed32f4a08b2a1390d607d8b3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fed07ec0c049402997045b96001a1bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RasyaAkbar/learning-py-torch/blob/main/06.%20Training%20data%20using%20script%20and%20cell%20mode%20%2B%20Exercise%20(using%20argparse%20for%20script%20params).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06. Going Modular Part 1 (Cell Mode)"
      ],
      "metadata": {
        "id": "4XRKnyzys3lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get Data\n",
        "\n",
        "Start by downloading the pizza_steak_sushi dataset"
      ],
      "metadata": {
        "id": "SoJROdsFtJ0R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EiiqZ51s1VA",
        "outputId": "d2e3bdbc-5faa-4e5e-9869-abadc65bd326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Didn't find data/pizza_steak_sushi directory, creating one...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If image folder doesn't exist, download and prepare it\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "  print(f\"Didn't find {image_path} directory, creating one...\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download pizza, steak, sushi data\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "  request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "  print(\"Downloading pizza, steak, sushi data...\")\n",
        "  f.write(request.content)\n",
        "\n",
        "# Unzip pizza, steak, sushi data\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "  print(\"Unzipping pizza, steak, sushi data...\")\n",
        "  zip_ref.extractall(image_path)\n",
        "\n",
        "# Remove zip file (we already extract it)\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup train and testing paths\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIUHulp7tbf7",
        "outputId": "f9d4f4b3-fde0-4f18-a733-6e3d5ee3b0c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi/train'),\n",
              " PosixPath('data/pizza_steak_sushi/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "cflf4rl5wZ5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Create simple transform\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)), # 64 x 64 image\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Use ImageFolder to create dataset(s)\n",
        "train_data = datasets.ImageFolder(root=train_dir, # target folder\n",
        "                                  transform=data_transform, # transform data\n",
        "                                  target_transform=None) # transform label (None since we use dir name as label)\n",
        "\n",
        "# Use ImageFolder to create dataset(s)\n",
        "test_data = datasets.ImageFolder(root=test_dir, # target folder\n",
        "                                  transform=data_transform, # transform data\n",
        "                                  target_transform=None) # transform label (None since we use dir name as label)\n",
        "\n",
        "\n",
        "print(f\"Train data: \\n{train_data} \\nTest data: \\n{test_data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0G-bvfzwTpl",
        "outputId": "3393825b-cba0-422a-bd01-c22e004c5620"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: \n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 225\n",
            "    Root location: data/pizza_steak_sushi/train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "           ) \n",
            "Test data: \n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 75\n",
            "    Root location: data/pizza_steak_sushi/test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class names as a list\n",
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy0hafSSx2Td",
        "outputId": "3677c8b1-92d6-4a55-e49f-58f00940cde9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pizza', 'steak', 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class names as dict\n",
        "class_dict = train_data.class_to_idx\n",
        "class_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrtIco0Bx-NF",
        "outputId": "5f17b64b-1d14-4de1-f957-d2c84cb7a930"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pizza': 0, 'steak': 1, 'sushi': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nhqs7hAyJFc",
        "outputId": "5c593705-62ea-4db4-fdeb-c4fe26008fe5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(225, 75)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn train and test datasets into dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=1, # how many samples per batch\n",
        "                              num_workers=os.cpu_count(), # how many subprocesses to use for data loading?\n",
        "                              shuffle=True # Shuffle data for training\n",
        "                              )\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                              batch_size=1, # how many samples per batch\n",
        "                              num_workers=os.cpu_count(), # how many subprocesses to use for data loading?\n",
        "                              shuffle=False # Not Shuffle data for test\n",
        "                              )\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43Dk9tSsyNHB",
        "outputId": "0212ee1a-68e4-49e9-8340-0687d9e14754"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7f6d379cb3a0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7f6d379c8460>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkout for single image\n",
        "img, label = next(iter(train_dataloader))\n",
        "\n",
        "img.shape, label.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDBcUxYmzlEa",
        "outputId": "9d329b43-b66b-4555-9149-b7715c42750a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 64, 64]), torch.Size([1]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Create Datasets and DataLoaders (script mode)\n",
        "\n",
        "Create a `.py` file for creating DataLoaders\n",
        "\n",
        "we can save a code cell's contents to a file using Jupyter magic `%%writefile filename` - https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile\n",
        "\n"
      ],
      "metadata": {
        "id": "1vaBKMVT0InE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory going_modular scripts\n",
        "import os\n",
        "os.makedirs(\"going_modular\")"
      ],
      "metadata": {
        "id": "c5a5lxbqz16g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "\"\"\"\n",
        "Contains functionality for creating PyTorch DataLoader's for image\n",
        "classification data.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int = NUM_WORKERS\n",
        "):\n",
        "  \"\"\"Creates training and testing DataLoaders.\n",
        "\n",
        "  Takes in a training directory and testing directroy path and turns them into\n",
        "  PyTorch Datasets and then into PyTorch DataLoaders.\n",
        "\n",
        "  Args:\n",
        "    train_dir: Path to training directory.\n",
        "    test_dir: Path to testing directory.\n",
        "    transform: torchvision transforms to perform on training and testing data.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "    Where class_names is a list of the target classes.\n",
        "    Example usage:\n",
        "      train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=path/to/train_dir,\n",
        "        test_dir=path/to/test_dir,\n",
        "        transform=some_transform,\n",
        "        batch_size=32,\n",
        "        num_workers=4)\n",
        "  \"\"\"\n",
        "\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(root=train_dir, # target folder\n",
        "                                    transform=transform, # transform data\n",
        "                                    target_transform=None) # transform label (None since we use dir name as label)\n",
        "\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  test_data = datasets.ImageFolder(root=test_dir, # target folder\n",
        "                                    transform=transform, # transform data\n",
        "                                    target_transform=None) # transform label (None since we use dir name as label)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into DataLoaders\n",
        "  train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=batch_size, # how many samples per batch\n",
        "                              num_workers=num_workers, # how many subprocesses to use for data loading?\n",
        "                              shuffle=True, # Shuffle data for training\n",
        "                              pin_memory=True # If True, the data loader will copy Tensors into device/CUDA pinned memory before returning them\n",
        "                              )\n",
        "  test_dataloader = DataLoader(dataset=test_data,\n",
        "                                batch_size=batch_size, # how many samples per batch\n",
        "                                num_workers=num_workers, # how many subprocesses to use for data loading?\n",
        "                                shuffle=False, # Not Shuffle data for test\n",
        "                                pin_memory=True # If True, the data loader will copy Tensors into device/CUDA pinned memory before returning them\n",
        "                                )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d9f-sGE1E03",
        "outputId": "601b2fd5-f876-4cfe-bb77-c656d002cab7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular import data_setup\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir= train_dir,\n",
        "                                                                              test_dir= test_dir,\n",
        "                                                                              transform=data_transform,\n",
        "                                                                              batch_size= 32)\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mXNgKRaFbIM",
        "outputId": "aa2ce669-bfbd-470a-b61a-9fa947c980f2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7f6d36848430>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7f6d368493c0>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Making a model (TinyVGG)"
      ],
      "metadata": {
        "id": "igc6ES4JGZtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "same model as notebook 4 but added a docstring using this guide: https://google.github.io/styleguide/pyguide.html#384-classes"
      ],
      "metadata": {
        "id": "P-8N6fYlJ974"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  \"\"\"\n",
        "  Model architecture that replicates the TinyVGG\n",
        "  model from CNN explainer website  https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "  Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        # Create conv layer\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2\n",
        "                     ), # stride by default is the same as kernel size\n",
        "        #nn.Dropout(p=0.5)#   Dropout with a 70% drop rate\n",
        "    )\n",
        "\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        # Create conv layer\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2\n",
        "                     ), # stride by default is the same as kernel size\n",
        "        nn.Dropout(p=0.5) #  Dropout with a 50% drop rate\n",
        "    )\n",
        "    self.conv_block_3 = nn.Sequential(\n",
        "        # Create conv layer\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=0 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=0 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2\n",
        "                     ), # stride by default is the same as kernel size\n",
        "        nn.Dropout(p=0.5)  # Dropout with a 70% drop rate\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*256, out_features=output_shape) # 7*7 = 49 which is the shape when the image compressed is flatten\n",
        "    )\n",
        "  def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
        "    return self.classifier(self.conv_block_2(self.conv_block_1(x))) #benefited from operator fusion which behind the scene speeds up how GPU perform computation cuz its 1 step\n",
        "    # https://horace.io/brrr_intro.html"
      ],
      "metadata": {
        "id": "MFq4E4VHGWqJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate an instance of the model\n",
        "torch.manual_seed(42)\n",
        "model_0 = TinyVGG(input_shape=3, # Number of color channels\n",
        "                  hidden_units=10,\n",
        "                  output_shape=len(train_data.classes)\n",
        "                  ).to(device)\n",
        "model_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCRAiYn0HvXc",
        "outputId": "755d47dc-ba30-4c72-8818-028f112eaa7d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyVGG(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (conv_block_3): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=2560, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model to findout hidden units multiplier"
      ],
      "metadata": {
        "id": "DoygwwPSIYxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0(torch.rand(32, 3, 64, 64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpgTMoJVJJiD",
        "outputId": "5df5d756-e72e-4444-e30e-31194e0ba48e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1059,  0.0129, -0.0421],\n",
              "        [ 0.1535,  0.0974, -0.0370],\n",
              "        [ 0.0771,  0.0868, -0.0420],\n",
              "        [ 0.0931,  0.0689, -0.0556],\n",
              "        [ 0.1371,  0.0842, -0.0777],\n",
              "        [ 0.1921,  0.0949, -0.0642],\n",
              "        [ 0.1087,  0.0879, -0.0184],\n",
              "        [ 0.0616,  0.0558,  0.0500],\n",
              "        [ 0.1418,  0.0835, -0.0375],\n",
              "        [ 0.0674,  0.0551,  0.0418],\n",
              "        [ 0.0371,  0.0586, -0.0039],\n",
              "        [ 0.1247, -0.0251, -0.0630],\n",
              "        [ 0.1439,  0.0608, -0.0080],\n",
              "        [ 0.0777, -0.0011, -0.0322],\n",
              "        [ 0.0924,  0.0426, -0.0968],\n",
              "        [ 0.1371,  0.0066,  0.0206],\n",
              "        [ 0.1536,  0.0305, -0.0034],\n",
              "        [ 0.0306,  0.0519, -0.0604],\n",
              "        [ 0.1213, -0.0399, -0.0572],\n",
              "        [ 0.0982,  0.0312, -0.0471],\n",
              "        [ 0.1145,  0.0631, -0.0237],\n",
              "        [ 0.0958, -0.0088, -0.0083],\n",
              "        [ 0.1022,  0.0277, -0.0432],\n",
              "        [ 0.0733,  0.0229, -0.0520],\n",
              "        [ 0.0052,  0.0044,  0.0320],\n",
              "        [ 0.0649, -0.0048, -0.0271],\n",
              "        [ 0.1781,  0.0445,  0.0084],\n",
              "        [ 0.0864,  0.0008, -0.0856],\n",
              "        [ 0.0782,  0.0077,  0.0071],\n",
              "        [ 0.0574,  0.0242,  0.0126],\n",
              "        [ 0.1418,  0.0342, -0.0018],\n",
              "        [ 0.0686,  0.0485,  0.0003]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Making a model (TinyVGG) with a script (`model_builder.py`)"
      ],
      "metadata": {
        "id": "yBUNO4SgJntY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets turn our model building code into Python script so we can import"
      ],
      "metadata": {
        "id": "cpL6ywIkKW-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_builder.py\n",
        "\"\"\"\n",
        "Contains PyTorch model code to instantiate a TinyVGG model from CNN explainer website\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  \"\"\"\n",
        "  Model architecture that replicates the TinyVGG\n",
        "  model from CNN explainer website  https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "  Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        # Create conv layer\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2\n",
        "                     ), # stride by default is the same as kernel size\n",
        "        #nn.Dropout(p=0.5)#   Dropout with a 70% drop rate\n",
        "    )\n",
        "\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        # Create conv layer\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2\n",
        "                     ), # stride by default is the same as kernel size\n",
        "        nn.Dropout(p=0.5) #  Dropout with a 50% drop rate\n",
        "    )\n",
        "    self.conv_block_3 = nn.Sequential(\n",
        "        # Create conv layer\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=0 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=0 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2\n",
        "                     ), # stride by default is the same as kernel size\n",
        "        nn.Dropout(p=0.5)  # Dropout with a 70% drop rate\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*256, out_features=output_shape) # 7*7 = 49 which is the shape when the image compressed is flatten\n",
        "    )\n",
        "  def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
        "    return self.classifier(self.conv_block_2(self.conv_block_1(x))) #benefited from operator fusion which behind the scene speeds up how GPU perform computation cuz its 1 step\n",
        "    # https://horace.io/brrr_intro.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK19VlFrJUaR",
        "outputId": "e17c0a8b-646b-4281-b002-2118fd5a9c96"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular import model_builder"
      ],
      "metadata": {
        "id": "Dfx7V7nyLZO8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from going_modular import model_builder\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate a model from the model_builder.py script\n",
        "torch.manual_seed(42)\n",
        "model_1 = model_builder.TinyVGG(input_shape=3,\n",
        "                                hidden_units=10,\n",
        "                                output_shape=len(class_names)\n",
        "                                ).to(device)\n",
        "model_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS-OmOUDLd4x",
        "outputId": "99dfd0ed-e57b-4f78-cdba-f053675c74a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyVGG(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (conv_block_3): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=2560, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1(torch.rand(32, 3, 64, 64)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nctfqD7vMOxZ",
        "outputId": "77fd736c-1a92-4208-879e-e535bc84804c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Creating `train_step()` and `test_step()` functions and `train()` to combine them"
      ],
      "metadata": {
        "id": "OkkE_U1RMwZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple"
      ],
      "metadata": {
        "id": "Y7UO-boqOrh6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device = device\n",
        "               ):\n",
        "  \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forward\n",
        "  pass, loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "  \"\"\"\n",
        "  ### Training\n",
        "  train_loss, train_acc = 0, 0\n",
        "  # Put model into training model\n",
        "  model.train()\n",
        "\n",
        "  # Add a loop to loop through the training batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # 1. Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    l1_lambda = 0.001\n",
        "    l1_norm = sum(p.abs().sum() for p in model.parameters()) # L1 reg\n",
        "\n",
        "    #2. Calculate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item() # Accumulate train loss\n",
        "\n",
        "    # Calculate accuracy metric\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc+= ((y_pred_class==y).sum().item()/len(y_pred)) * 100\n",
        "\n",
        "    #3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    #5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    #if(batch % 2 == 0):\n",
        "    #  print(f\"Looked at {batch * len(X)} samples out of {len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "  # Divide total train loss by length of train dataloader\n",
        "  train_loss /= len(dataloader) # Average loss per batch\n",
        "\n",
        "  # Average accuracy per batch\n",
        "  train_acc /= len(dataloader)\n",
        "\n",
        "  print(f\"\\nTrain loss: {train_loss:.4f} , Train accuracy: {train_acc:.4f}% \")\n",
        "  return train_loss, train_acc\n",
        ""
      ],
      "metadata": {
        "id": "xeMJ3CgmMZmm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               device: torch.device = device\n",
        "              ):\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "  \"\"\"\n",
        "  ### Testing\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test, y_test in dataloader:\n",
        "      X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      test_pred_logits = model(X_test)\n",
        "\n",
        "      #2. Calculate loss\n",
        "      loss = loss_fn(test_pred_logits, y_test)\n",
        "      test_loss += loss.item() # Accumulate test loss\n",
        "\n",
        "      # Calculate accuracy metric\n",
        "      #test_pred_labels = torch.argmax(torch.softmax(test_pred_logits, dim=1), dim=1)\n",
        "      #test_acc+= ((test_pred_labels==y_test).sum().item()/len(test_pred_logits)) * 100\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1) # another way of doing it (?) further testing required\n",
        "      test_acc+= ((test_pred_labels==y_test).sum().item()/len(test_pred_labels)) * 100\n",
        "\n",
        "    # Average loss per batch\n",
        "    test_loss /= len(dataloader)\n",
        "\n",
        "    # Average accuracy per batch\n",
        "    test_acc /= len(dataloader)\n",
        "\n",
        "  print(f\"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}%\\n\")\n",
        "  return test_loss, test_acc"
      ],
      "metadata": {
        "id": "4z8euWyCNWEs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "# tqdm.auto recognize computer enviroment we use and give the best type progress bar, ex: jupyter notebook bar differ from python script\n",
        "from tqdm.auto import tqdm\n",
        "def train(model: torch.nn.Module,\n",
        "            train_dataloader: torch.utils.data.DataLoader,\n",
        "            test_dataloader: torch.utils.data.DataLoader,\n",
        "            loss_fn: torch.nn.Module,\n",
        "            optimizer: torch.optim.Optimizer,\n",
        "            device: torch.device = device,\n",
        "            epochs: int = 5\n",
        "          ):\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Passes a target PyTorch models through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "                  train_acc: [...],\n",
        "                  test_loss: [...],\n",
        "                  test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "                 {train_loss: [2.0616, 1.0537],\n",
        "                  train_acc: [0.3945, 0.3945],\n",
        "                  test_loss: [1.2641, 1.5706],\n",
        "                  test_acc: [0.3400, 0.2973]}\n",
        "  \"\"\"\n",
        "  results = {\n",
        "      \"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  # Create training and test loop\n",
        "  for epoch in tqdm(range(epochs)): # the way tqdm works is to wrap our iterator in tqdm\n",
        "    print(f\"Epoch: {epoch}\\n-----\")\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "               dataloader=train_dataloader,\n",
        "               loss_fn=loss_fn,\n",
        "               optimizer=optimizer)\n",
        "    model.eval()\n",
        "    test_loss, test_acc = test_step(model=model,\n",
        "              dataloader=test_dataloader,\n",
        "              loss_fn=loss_fn)\n",
        "\n",
        "    # Update result dictionary\n",
        "    train_loss = train_loss\n",
        "    #print(train_loss, train_acc, test_loss, test_acc)\n",
        "    results[\"train_loss\"].append(train_loss)#.cpu()\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)#.cpu()\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "ybXr08XVNwaq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Turn training functions into a script (`engine.py`)"
      ],
      "metadata": {
        "id": "TvDMWyvsQPYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "# Import tqdm for progress bar\n",
        "# tqdm.auto recognize computer enviroment we use and give the best type progress bar, ex: jupyter notebook bar differ from python script\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device = device\n",
        "               ):\n",
        "  \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forward\n",
        "  pass, loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "  \"\"\"\n",
        "  ### Training\n",
        "  train_loss, train_acc = 0, 0\n",
        "  # Put model into training model\n",
        "  model.train()\n",
        "\n",
        "  # Add a loop to loop through the training batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # 1. Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    l1_lambda = 0.001\n",
        "    l1_norm = sum(p.abs().sum() for p in model.parameters()) # L1 reg\n",
        "\n",
        "    #2. Calculate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item() # Accumulate train loss\n",
        "\n",
        "    # Calculate accuracy metric\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc+= ((y_pred_class==y).sum().item()/len(y_pred)) * 100\n",
        "\n",
        "    #3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    #5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    #if(batch % 2 == 0):\n",
        "    #  print(f\"Looked at {batch * len(X)} samples out of {len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "  # Divide total train loss by length of train dataloader\n",
        "  train_loss /= len(dataloader) # Average loss per batch\n",
        "\n",
        "  # Average accuracy per batch\n",
        "  train_acc /= len(dataloader)\n",
        "\n",
        "  print(f\"\\nTrain loss: {train_loss:.4f} , Train accuracy: {train_acc:.4f}% \")\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               device: torch.device = device\n",
        "              ):\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "  \"\"\"\n",
        "  ### Testing\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test, y_test in dataloader:\n",
        "      X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      test_pred_logits = model(X_test)\n",
        "\n",
        "      #2. Calculate loss\n",
        "      loss = loss_fn(test_pred_logits, y_test)\n",
        "      test_loss += loss.item() # Accumulate test loss\n",
        "\n",
        "      # Calculate accuracy metric\n",
        "      #test_pred_labels = torch.argmax(torch.softmax(test_pred_logits, dim=1), dim=1)\n",
        "      #test_acc+= ((test_pred_labels==y_test).sum().item()/len(test_pred_logits)) * 100\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1) # another way of doing it (?) further testing required\n",
        "      test_acc+= ((test_pred_labels==y_test).sum().item()/len(test_pred_labels)) * 100\n",
        "\n",
        "    # Average loss per batch\n",
        "    test_loss /= len(dataloader)\n",
        "\n",
        "    # Average accuracy per batch\n",
        "    test_acc /= len(dataloader)\n",
        "\n",
        "  print(f\"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}%\\n\")\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "            train_dataloader: torch.utils.data.DataLoader,\n",
        "            test_dataloader: torch.utils.data.DataLoader,\n",
        "            loss_fn: torch.nn.Module,\n",
        "            optimizer: torch.optim.Optimizer,\n",
        "            device: torch.device = device,\n",
        "            epochs: int = 5\n",
        "          ):\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Passes a target PyTorch models through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "                  train_acc: [...],\n",
        "                  test_loss: [...],\n",
        "                  test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "                 {train_loss: [2.0616, 1.0537],\n",
        "                  train_acc: [0.3945, 0.3945],\n",
        "                  test_loss: [1.2641, 1.5706],\n",
        "                  test_acc: [0.3400, 0.2973]}\n",
        "  \"\"\"\n",
        "  results = {\n",
        "      \"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  # Create training and test loop\n",
        "  for epoch in tqdm(range(epochs)): # the way tqdm works is to wrap our iterator in tqdm\n",
        "    print(f\"Epoch: {epoch}\\n-----\")\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "               dataloader=train_dataloader,\n",
        "               loss_fn=loss_fn,\n",
        "               optimizer=optimizer)\n",
        "    model.eval()\n",
        "    test_loss, test_acc = test_step(model=model,\n",
        "              dataloader=test_dataloader,\n",
        "              loss_fn=loss_fn)\n",
        "\n",
        "    # Update result dictionary\n",
        "    train_loss = train_loss\n",
        "    #print(train_loss, train_acc, test_loss, test_acc)\n",
        "    results[\"train_loss\"].append(train_loss)#.cpu()\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)#.cpu()\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "\n",
        "  return results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNe5UiFzQCOq",
        "outputId": "982b1a5e-38fe-4e11-b746-f70c349437b6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular import engine"
      ],
      "metadata": {
        "id": "XTMd1sEeRscv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Creating a function to save the model\n",
        "\n",
        "Let's setup a function to save our model to a directory"
      ],
      "metadata": {
        "id": "9FUbMezcSAwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str\n",
        "               ):\n",
        "  \"\"\"Saves a PyTorch model to a target directory.\n",
        "\n",
        "  Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "\n",
        "  Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "  \"\"\"\n",
        "  # Create target directory\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True\n",
        "                        )\n",
        "\n",
        "  # Create model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with .pt or .pth\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  # Save the model state_dict()\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path\n",
        "             )"
      ],
      "metadata": {
        "id": "A2HRhLetRxk9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Create a file called `utils.py` with utility functions\n",
        "\n",
        "\"utils\" in Python is generally reserved for various utility functions\n",
        "\n",
        "Right now we only have one utility function (`save_model()`) but as our code grows we likely have more..."
      ],
      "metadata": {
        "id": "OwboapZnUmVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/utils.py\n",
        "\"\"\"\n",
        "File containing various utility functions for PyTorch model training\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str\n",
        "               ):\n",
        "  \"\"\"Saves a PyTorch model to a target directory.\n",
        "\n",
        "  Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "\n",
        "  Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "  \"\"\"\n",
        "  # Create target directory\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True\n",
        "                        )\n",
        "\n",
        "  # Create model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with .pt or .pth\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  # Save the model state_dict()\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path\n",
        "             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no8XIAnhUkL6",
        "outputId": "02817f62-01a9-45bc-edf1-96704a8c3ef4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train, evaluate and save the model\n",
        "\n",
        "Lets leverage the functions we've got above to train, test and save a model to a file"
      ],
      "metadata": {
        "id": "ul1W-USpVd4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Recreate an instance of TinyVGG\n",
        "model_0 = TinyVGG(input_shape=3,\n",
        "                                hidden_units=10,\n",
        "                                output_shape=len(class_names)\n",
        "                                ).to(device)\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Train model_0\n",
        "model_0_results = train(model=model_0,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        device=device)\n",
        "\n",
        "# End timer and print how long it took to train and test\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n",
        "\n",
        "# Save the model\n",
        "save_model(model=model_0,\n",
        "           target_dir=\"models\",\n",
        "           model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624,
          "referenced_widgets": [
            "edd083b1715b4a3983a6f0518ba9dbc2",
            "8855c4298fda405ba18f500c29213415",
            "2024df471218498792b2663e764adce5",
            "523c78657bd94722941e9c81f7f6cd28",
            "6eac04bae33447b5a655964794d3e0d4",
            "64349dee113941e8bfd52e1117505506",
            "4d28b08ab35d4418821a31ff496b2983",
            "fddb11daf2dc44f8b5ada8eb5246cf9b",
            "f83c77e8e92e4c4a97b5e89378d0e22f",
            "3ca5ef4ed32f4a08b2a1390d607d8b3b",
            "fed07ec0c049402997045b96001a1bbc"
          ]
        },
        "id": "S2B4Skf8VcWx",
        "outputId": "84338fa0-01d3-43b2-e8ed-d507513e3a1f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edd083b1715b4a3983a6f0518ba9dbc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "-----\n",
            "\n",
            "Train loss: 1.1156 , Train accuracy: 26.9531% \n",
            "Test loss: 1.1084, Test accuracy: 26.0417%\n",
            "\n",
            "Epoch: 1\n",
            "-----\n",
            "\n",
            "Train loss: 1.0940 , Train accuracy: 41.4062% \n",
            "Test loss: 1.0824, Test accuracy: 54.1667%\n",
            "\n",
            "Epoch: 2\n",
            "-----\n",
            "\n",
            "Train loss: 1.1085 , Train accuracy: 28.9062% \n",
            "Test loss: 1.0632, Test accuracy: 54.1667%\n",
            "\n",
            "Epoch: 3\n",
            "-----\n",
            "\n",
            "Train loss: 1.0952 , Train accuracy: 37.8906% \n",
            "Test loss: 1.0779, Test accuracy: 54.1667%\n",
            "\n",
            "Epoch: 4\n",
            "-----\n",
            "\n",
            "Train loss: 1.0941 , Train accuracy: 27.3438% \n",
            "Test loss: 1.0732, Test accuracy: 48.2955%\n",
            "\n",
            "[INFO] Total training time: 12.429 seconds\n",
            "[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Train evaluate and save the model (script mode) -> `train.py`"
      ],
      "metadata": {
        "id": "lR4h6WZ58uZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train.py\n",
        "\"\"\"\n",
        "Trains a PyTorch image classification model using device agnostic code\n",
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from torchvision import transforms\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import data_setup, engine, model_builder, utils\n",
        "\n",
        "# Setup hyperparameters\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "HIDDEN_UNITS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Setup directories\n",
        "train_dir = \"data/pizza_steak_sushi/train\"\n",
        "test_dir = \"data/pizza_steak_sushi/test\"\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create transforms\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create DataLoader's and get class_names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=data_transform,\n",
        "                                                                               batch_size=BATCH_SIZE)\n",
        "\n",
        "# Create model\n",
        "model = model_builder.TinyVGG(input_shape=3,\n",
        "                              hidden_units=HIDDEN_UNITS,\n",
        "                              output_shape=len(class_names)\n",
        "                              ).to(device)\n",
        "\n",
        "# Setup loss and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE\n",
        "                             )\n",
        "\n",
        "# Start the timer\n",
        "start_time = timer()\n",
        "\n",
        "# Start training with help from engine.py\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=NUM_EPOCHS,\n",
        "             device=device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n",
        "\n",
        "# Save the model to file\n",
        "utils.save_model(model=model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz_VjQTcXrWa",
        "outputId": "7a423e9c-0bd0-4254-c909-840f1b130419"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/train.py # Run the script"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9LRmeDD_8xh",
        "outputId": "6170cbaf-9543-4152-d503-143f25123582"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/5 [00:00<?, ?it/s]Epoch: 0\n",
            "-----\n",
            "\n",
            "Train loss: 1.1102 , Train accuracy: 26.5625% \n",
            "Test loss: 1.1047, Test accuracy: 35.4167%\n",
            "\n",
            " 20% 1/5 [00:02<00:11,  2.95s/it]Epoch: 1\n",
            "-----\n",
            "\n",
            "Train loss: 1.0853 , Train accuracy: 42.5781% \n",
            "Test loss: 1.1328, Test accuracy: 19.7917%\n",
            "\n",
            " 40% 2/5 [00:06<00:09,  3.10s/it]Epoch: 2\n",
            "-----\n",
            "\n",
            "Train loss: 1.0991 , Train accuracy: 29.2969% \n",
            "Test loss: 1.1416, Test accuracy: 19.7917%\n",
            "\n",
            " 60% 3/5 [00:09<00:06,  3.14s/it]Epoch: 3\n",
            "-----\n",
            "\n",
            "Train loss: 1.0727 , Train accuracy: 45.7031% \n",
            "Test loss: 1.1271, Test accuracy: 25.0000%\n",
            "\n",
            " 80% 4/5 [00:11<00:02,  2.73s/it]Epoch: 4\n",
            "-----\n",
            "\n",
            "Train loss: 1.0749 , Train accuracy: 34.7656% \n",
            "Test loss: 1.1175, Test accuracy: 21.8750%\n",
            "\n",
            "100% 5/5 [00:13<00:00,  2.70s/it]\n",
            "[INFO] Total training time: 13.488 seconds\n",
            "[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n"
      ],
      "metadata": {
        "id": "cpIpDrfMC5es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory exercise scripts\n",
        "import os\n",
        "os.makedirs(\"exercise\")"
      ],
      "metadata": {
        "id": "FFPjQZtGETCf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exercise/get_data.py\n",
        "\"\"\"\n",
        "Get image data from github repo\n",
        "\"\"\"\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If image folder doesn't exist, download and prepare it\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "  print(f\"Didn't find {image_path} directory, creating one...\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download pizza, steak, sushi data\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "  request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "  print(\"Downloading pizza, steak, sushi data...\")\n",
        "  f.write(request.content)\n",
        "\n",
        "# Unzip pizza, steak, sushi data\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "  print(\"Unzipping pizza, steak, sushi data...\")\n",
        "  zip_ref.extractall(image_path)\n",
        "\n",
        "# Remove zip file (we already extract it)\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PGEjCcWAXx6",
        "outputId": "8b2c240f-c75f-470b-d96c-0227db18512a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing exercise/get_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python exercise/get_data.py # Run the script"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAMuSW8IEMGQ",
        "outputId": "fddcfdb9-e2c1-4dd6-c063-e29a04596f5d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pizza_steak_sushi directory exists.\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exercise/data_setup.py\n",
        "\"\"\"\n",
        "Contains functionality for creating PyTorch DataLoader's for image\n",
        "classification data.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int = NUM_WORKERS\n",
        "):\n",
        "  \"\"\"Creates training and testing DataLoaders.\n",
        "\n",
        "  Takes in a training directory and testing directroy path and turns them into\n",
        "  PyTorch Datasets and then into PyTorch DataLoaders.\n",
        "\n",
        "  Args:\n",
        "    train_dir: Path to training directory.\n",
        "    test_dir: Path to testing directory.\n",
        "    transform: torchvision transforms to perform on training and testing data.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "    Where class_names is a list of the target classes.\n",
        "    Example usage:\n",
        "      train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=path/to/train_dir,\n",
        "        test_dir=path/to/test_dir,\n",
        "        transform=some_transform,\n",
        "        batch_size=32,\n",
        "        num_workers=4)\n",
        "  \"\"\"\n",
        "\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(root=train_dir, # target folder\n",
        "                                    transform=transform, # transform data\n",
        "                                    target_transform=None) # transform label (None since we use dir name as label)\n",
        "\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  test_data = datasets.ImageFolder(root=test_dir, # target folder\n",
        "                                    transform=transform, # transform data\n",
        "                                    target_transform=None) # transform label (None since we use dir name as label)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into DataLoaders\n",
        "  train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=batch_size, # how many samples per batch\n",
        "                              num_workers=num_workers, # how many subprocesses to use for data loading?\n",
        "                              shuffle=True, # Shuffle data for training\n",
        "                              pin_memory=True # If True, the data loader will copy Tensors into device/CUDA pinned memory before returning them\n",
        "                              )\n",
        "  test_dataloader = DataLoader(dataset=test_data,\n",
        "                                batch_size=batch_size, # how many samples per batch\n",
        "                                num_workers=num_workers, # how many subprocesses to use for data loading?\n",
        "                                shuffle=False, # Not Shuffle data for test\n",
        "                                pin_memory=True # If True, the data loader will copy Tensors into device/CUDA pinned memory before returning them\n",
        "                                )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aMxoTc0EeSh",
        "outputId": "1a24a5a2-8860-4671-b67d-afb6cf2c3db8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing exercise/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exercise/model_builder.py\n",
        "\"\"\"\n",
        "Contains PyTorch model code to instantiate a TinyVGG model from CNN explainer website\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  \"\"\"\n",
        "  Model architecture that replicates the TinyVGG\n",
        "  model from CNN explainer website  https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "  Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        # Create conv layer\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2\n",
        "                     ), # stride by default is the same as kernel size\n",
        "        #nn.Dropout(p=0.5)#   Dropout with a 70% drop rate\n",
        "    )\n",
        "\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        # Create conv layer\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=1 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2\n",
        "                     ), # stride by default is the same as kernel size\n",
        "        nn.Dropout(p=0.5) #  Dropout with a 50% drop rate\n",
        "    )\n",
        "    self.conv_block_3 = nn.Sequential(\n",
        "        # Create conv layer\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=0 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3, # often also referred to as filter size, refers to the dimensions of the sliding window over the input. can be tuple, ex:(3, 3)\n",
        "                  stride=1, # indicates how many pixels the kernel should be shifted over at a time.\n",
        "                  padding=0 # By adding padding, the model retains information from the edges of the input data, which would otherwise be lost without padding.\n",
        "                  ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2\n",
        "                     ), # stride by default is the same as kernel size\n",
        "        nn.Dropout(p=0.5)  # Dropout with a 70% drop rate\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*256, out_features=output_shape) # 7*7 = 49 which is the shape when the image compressed is flatten\n",
        "    )\n",
        "  def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
        "    return self.classifier(self.conv_block_2(self.conv_block_1(x))) #benefited from operator fusion which behind the scene speeds up how GPU perform computation cuz its 1 step\n",
        "    # https://horace.io/brrr_intro.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQAkCEKMGOWS",
        "outputId": "e2eef758-c694-4c9b-80c5-c4318162093e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing exercise/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exercise/engine.py\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "# Import tqdm for progress bar\n",
        "# tqdm.auto recognize computer enviroment we use and give the best type progress bar, ex: jupyter notebook bar differ from python script\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device = device\n",
        "               ):\n",
        "  \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forward\n",
        "  pass, loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "  \"\"\"\n",
        "  ### Training\n",
        "  train_loss, train_acc = 0, 0\n",
        "  # Put model into training model\n",
        "  model.train()\n",
        "\n",
        "  # Add a loop to loop through the training batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # 1. Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    l1_lambda = 0.001\n",
        "    l1_norm = sum(p.abs().sum() for p in model.parameters()) # L1 reg\n",
        "\n",
        "    #2. Calculate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item() # Accumulate train loss\n",
        "\n",
        "    # Calculate accuracy metric\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc+= ((y_pred_class==y).sum().item()/len(y_pred)) * 100\n",
        "\n",
        "    #3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    #5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    #if(batch % 2 == 0):\n",
        "    #  print(f\"Looked at {batch * len(X)} samples out of {len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "  # Divide total train loss by length of train dataloader\n",
        "  train_loss /= len(dataloader) # Average loss per batch\n",
        "\n",
        "  # Average accuracy per batch\n",
        "  train_acc /= len(dataloader)\n",
        "\n",
        "  print(f\"\\nTrain loss: {train_loss:.4f} , Train accuracy: {train_acc:.4f}% \")\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               device: torch.device = device\n",
        "              ):\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "  \"\"\"\n",
        "  ### Testing\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test, y_test in dataloader:\n",
        "      X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      test_pred_logits = model(X_test)\n",
        "\n",
        "      #2. Calculate loss\n",
        "      loss = loss_fn(test_pred_logits, y_test)\n",
        "      test_loss += loss.item() # Accumulate test loss\n",
        "\n",
        "      # Calculate accuracy metric\n",
        "      #test_pred_labels = torch.argmax(torch.softmax(test_pred_logits, dim=1), dim=1)\n",
        "      #test_acc+= ((test_pred_labels==y_test).sum().item()/len(test_pred_logits)) * 100\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1) # another way of doing it (?) further testing required\n",
        "      test_acc+= ((test_pred_labels==y_test).sum().item()/len(test_pred_labels)) * 100\n",
        "\n",
        "    # Average loss per batch\n",
        "    test_loss /= len(dataloader)\n",
        "\n",
        "    # Average accuracy per batch\n",
        "    test_acc /= len(dataloader)\n",
        "\n",
        "  print(f\"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}%\\n\")\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "            train_dataloader: torch.utils.data.DataLoader,\n",
        "            test_dataloader: torch.utils.data.DataLoader,\n",
        "            loss_fn: torch.nn.Module,\n",
        "            optimizer: torch.optim.Optimizer,\n",
        "            device: torch.device = device,\n",
        "            epochs: int = 5\n",
        "          ):\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Passes a target PyTorch models through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "                  train_acc: [...],\n",
        "                  test_loss: [...],\n",
        "                  test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "                 {train_loss: [2.0616, 1.0537],\n",
        "                  train_acc: [0.3945, 0.3945],\n",
        "                  test_loss: [1.2641, 1.5706],\n",
        "                  test_acc: [0.3400, 0.2973]}\n",
        "  \"\"\"\n",
        "  results = {\n",
        "      \"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  # Create training and test loop\n",
        "  for epoch in tqdm(range(epochs)): # the way tqdm works is to wrap our iterator in tqdm\n",
        "    print(f\"Epoch: {epoch}\\n-----\")\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "               dataloader=train_dataloader,\n",
        "               loss_fn=loss_fn,\n",
        "               optimizer=optimizer)\n",
        "    model.eval()\n",
        "    test_loss, test_acc = test_step(model=model,\n",
        "              dataloader=test_dataloader,\n",
        "              loss_fn=loss_fn)\n",
        "\n",
        "    # Update result dictionary\n",
        "    train_loss = train_loss\n",
        "    #print(train_loss, train_acc, test_loss, test_acc)\n",
        "    results[\"train_loss\"].append(train_loss)#.cpu()\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)#.cpu()\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "\n",
        "  return results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gIZLBj-GmSP",
        "outputId": "7c471cec-a9d3-4bf6-e357-a3a014bd739e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing exercise/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exercise/utils.py\n",
        "\"\"\"\n",
        "File containing various utility functions for PyTorch model training\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str\n",
        "               ):\n",
        "  \"\"\"Saves a PyTorch model to a target directory.\n",
        "\n",
        "  Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "\n",
        "  Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "  \"\"\"\n",
        "  # Create target directory\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True\n",
        "                        )\n",
        "\n",
        "  # Create model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with .pt or .pth\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  # Save the model state_dict()\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path\n",
        "             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhiD3oakGko2",
        "outputId": "e6d8adec-5ec8-4b6f-c066-02f738abaa40"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing exercise/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exercise/train.py\n",
        "\"\"\"\n",
        "Trains a PyTorch image classification model using device agnostic code\n",
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from torchvision import transforms\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import data_setup, engine, model_builder, utils\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser(prog='exercise/train')\n",
        "\n",
        "# Define the expected arguments\n",
        "parser.add_argument('--learning_rate', type=float, help=\"learning rate\")\n",
        "parser.add_argument('--batch_size', type=int, help=\"batch size\")\n",
        "parser.add_argument('--num_epochs', type=int, help=\"num epochs\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Setup hyperparameters\n",
        "NUM_EPOCHS = args.num_epochs or 5\n",
        "BATCH_SIZE = args.batch_size or 32\n",
        "HIDDEN_UNITS = 10\n",
        "LEARNING_RATE = args.learning_rate or 0.001\n",
        "\n",
        "# Setup directories\n",
        "train_dir = \"data/pizza_steak_sushi/train\"\n",
        "test_dir = \"data/pizza_steak_sushi/test\"\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create transforms\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create DataLoader's and get class_names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=data_transform,\n",
        "                                                                               batch_size=BATCH_SIZE)\n",
        "\n",
        "# Create or load model\n",
        "if (os.path.isfile(\"models/05_going_modular_script_mode_tinyvgg_model.pth\")):\n",
        "  # To load a saved state_dict we have to instantiate a new instance of our model class\n",
        "  # Load the saved state_dict of model (this will update the new instance with updated parameters)\n",
        "  model = model_builder.TinyVGG(input_shape=3,\n",
        "                              hidden_units=HIDDEN_UNITS,\n",
        "                              output_shape=len(class_names)).to(device)\n",
        "  model.load_state_dict(torch.load(f=\"models/05_going_modular_script_mode_tinyvgg_model.pth\", weights_only=True))\n",
        "else:\n",
        "  model = model_builder.TinyVGG(input_shape=3,\n",
        "                              hidden_units=HIDDEN_UNITS,\n",
        "                              output_shape=len(class_names)\n",
        "                              ).to(device)\n",
        "\n",
        "# Setup loss and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE\n",
        "                             )\n",
        "\n",
        "# Start the timer\n",
        "start_time = timer()\n",
        "\n",
        "# Start training with help from engine.py\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=NUM_EPOCHS,\n",
        "             device=device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n",
        "\n",
        "# Save the model to file\n",
        "utils.save_model(model=model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPl1jGNCGivt",
        "outputId": "20793fa6-7980-4f75-8626-d45e4b52033d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting exercise/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python exercise/train.py --learning_rate 0.003 --batch_size 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNbsLeaXJbZL",
        "outputId": "eed67314-093c-413e-d652-40961ca639cb"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/5 [00:00<?, ?it/s]Epoch: 0\n",
            "-----\n",
            "\n",
            "Train loss: 0.5917 , Train accuracy: 77.6042% \n",
            "Test loss: 1.2866, Test accuracy: 40.1989%\n",
            "\n",
            " 20% 1/5 [00:02<00:08,  2.13s/it]Epoch: 1\n",
            "-----\n",
            "\n",
            "Train loss: 0.5768 , Train accuracy: 76.8466% \n",
            "Test loss: 1.2107, Test accuracy: 45.5256%\n",
            "\n",
            " 40% 2/5 [00:05<00:07,  2.59s/it]Epoch: 2\n",
            "-----\n",
            "\n",
            "Train loss: 0.4605 , Train accuracy: 82.4929% \n",
            "Test loss: 1.1903, Test accuracy: 35.6534%\n",
            "\n",
            " 60% 3/5 [00:07<00:05,  2.69s/it]Epoch: 3\n",
            "-----\n",
            "\n",
            "Train loss: 0.4743 , Train accuracy: 80.9777% \n",
            "Test loss: 1.3510, Test accuracy: 33.3097%\n",
            "\n",
            " 80% 4/5 [00:10<00:02,  2.50s/it]Epoch: 4\n",
            "-----\n",
            "\n",
            "Train loss: 0.3684 , Train accuracy: 86.3518% \n",
            "Test loss: 1.2393, Test accuracy: 38.6364%\n",
            "\n",
            "100% 5/5 [00:12<00:00,  2.44s/it]\n",
            "[INFO] Total training time: 12.204 seconds\n",
            "[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exercise/predict.py\n",
        "\"\"\"\n",
        "Predict a custom image using loaded model\n",
        "\"\"\"\n",
        "print(\"type(custom_image_transformed.numpy())\")\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from typing import Dict\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "def pred_and_plot_image(model: torch.nn.Module,\n",
        "                        image_path: str,\n",
        "                        class_names:Dict[int, str],\n",
        "                        transform: torchvision.transforms,\n",
        "                        device: torch.device = device\n",
        "                        ):\n",
        "  # Read in custom image\n",
        "\n",
        "  custom_image = torchvision.io.read_image(str(image_path)).type(torch.float32) / 255\n",
        "\n",
        "  # Transform target image\n",
        "  #custom_image_transformed = transform(custom_image)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    custom_image_pred = model(custom_image_transformed.unsqueeze(0).to(device)) # add 1 batch\n",
        "\n",
        "  # Turn logits into prediction probabities\n",
        "  custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
        "\n",
        "  # Turn prediction probabities into prediction label\n",
        "  custom_image_label=custom_image_pred_probs.argmax(dim=1)\n",
        "\n",
        "\n",
        "  plt.imshow(custom_image_transformed.numpy())\n",
        "  plt.title(f\"Prediction: {class_names[custom_image_label]} | Confidence: {custom_image_pred_probs.max()*100:.1f}%\")\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1HzZ2qyJJJE",
        "outputId": "264d4118-5e35-4443-f0cf-20d5e13416ae"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting exercise/predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from exercise import model_builder, predict\n",
        "from torchvision import transforms\n",
        "\n",
        "HIDDEN_UNITS = 10\n",
        "model = model_builder.TinyVGG(input_shape=3,\n",
        "                            hidden_units=HIDDEN_UNITS,\n",
        "                            output_shape=len(class_names)).to(device)\n",
        "model.load_state_dict(torch.load(f=\"models/05_going_modular_script_mode_tinyvgg_model.pth\", weights_only=True))\n",
        "\n",
        "# Create transforms\n",
        "custom_image_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64))\n",
        "])\n",
        "\n",
        "# Download custom image\n",
        "import requests\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n",
        "\n",
        "# Download the image if it doesnt exist\n",
        "if not custom_image_path.is_file():\n",
        "  with open(custom_image_path, \"wb\") as f: # Create file with write permissions as f (shorthand for file)\n",
        "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\") # make sure get the raw data, not blob\n",
        "    print(f\"Downloading Image...\")\n",
        "    f.write(request.content)\n",
        "else:\n",
        "  print(f\"{custom_image_path} already exist\")\n",
        "\n",
        "# Pred on our custom image\n",
        "predict.pred_and_plot_image(model=model,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names,\n",
        "                    transform=custom_image_transform,\n",
        "                    device=device)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "QSDt8iouK-o-",
        "outputId": "2e3ffec8-7824-4a2b-ad99-f87f06b8a7b4"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/04-pizza-dad.jpeg already exist\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABeGElEQVR4nO29eZRdZZnv/z3zUNOpSg1JZahMhJBACIRRQgAFwmgjCtirW5GriN0qaLfNtXvZTdOiLi/NbbjgQmy50WZSFBAcAKUBRZAEJAyBJGSeKql5PvM57+8PftmX4v0+cILYyOrvZy3+4Kk3e7/73e/eT516vuf7hJxzDkIIIQSA8Ls9ASGEEH86KCkIIYQIUFIQQggRoKQghBAiQElBCCFEgJKCEEKIACUFIYQQAUoKQgghApQUhBBCBCgp1Mjs2bPxiU98Ivj/xx9/HKFQCI8//vg7do5QKIR//ud/fseO91/FH2Mt3gn2z2v79u3v9lRq4tprr8XcuXMRiUSwdOlSAP6+s/je9773nrpW8afLeyIp7N/w+/9LJpNYsGABPve5z6Gnp+fdnt4B8Ytf/OI9+eL/70ClUsGqVatw8skno6WlBYlEArNnz8Yll1yCZ5999o967l/+8pe48sorccIJJ2DVqlX4+te//kc933uRRx55BKeccgpaW1uRyWRwzDHH4Lbbbps05o3vijf+d8cdd7zpOV5++WVccMEFmDt3LtLpNFpbW7FixQr89Kc/9cb+5Cc/wcKFC9HU1IRzzz0X3d3d3pgPfvCD+PSnP/2HXfh/MdF3ewIHwr/8y79gzpw5yOfz+O1vf4ubb74Zv/jFL7Bu3Tqk0+n/0rmsWLECuVwO8Xj8gP7dL37xC3zrW9+iiSGXyyEafU/dEgBvfy3+lMjlcjj//PPx0EMPYcWKFfiHf/gHtLS0YPv27bj77rvx/e9/Hzt37sSMGTP+KOd/9NFHEQ6Hceutt05ax40bNyIcfk/87vZH5YEHHsB5552H448/Hv/8z/+MUCiEu+++Gx//+MfR39+PL37xiwBe24tvTBQA8G//9m944YUX8IEPfOBNz7Njxw6MjY3h4osvRmdnJ7LZLO655x588IMfxC233BK84Ldu3YqLLroIF110EY4//nhcf/31uOSSS/Dwww8Hx3r44Yfxm9/8Bps2bXoHV+K/APceYNWqVQ6Ae+aZZybF/+Zv/sYBcHfeeaf5b8fHx9+ROXR1dbmLL774Dz7OZz/7WfceWfb3PI899pgD4LZt2/aWY/ffl3/7t3/zflYul921117rdu3a9c5P8v/nkksucXV1dW/73+9/Rmq51vcip512muvs7HT5fD6IlUolN2/ePLdkyZI3/bfZbNY1NDS400477W2du1wuu8MPP9wdfPDBQezmm292c+fOddVq1Tn32l4LhUIul8sFczvkkEPcdddd97bO+W7ynv4V5P3vfz8AYNu2bQCAT3ziE6ivr8eWLVtw1llnoaGhAX/xF38BAKhWq7j++uuxePFiJJNJdHR04LLLLsPQ0NCkYzrncM0112DGjBlIp9M45ZRT8PLLL3vntv6Ovnr1apx11llobm5GXV0dlixZghtuuCGY37e+9S0AmPSRdj+sprB27VqceeaZaGxsRH19PT7wgQ/g6aefnjRm/0fmJ598En/zN3+DtrY21NXV4UMf+hD6+vomjR0ZGcGGDRswMjLylus7e/ZsnHPOOfjlL3+JpUuXIplMYtGiRbj33nvfdC3e7CP8ySefHKyFNWb/GhSLRfzTP/0Tli1bhqamJtTV1eHEE0/EY4899pZzPxB2796NW265Baeddhq+8IUveD+PRCL40pe+NOlTwjt5X0KhEFatWoWJiYlgDb73ve8B4DWFl19+Ge9///uRSqUwY8YMXHPNNahWq/TaHnzwQZx44omoq6tDQ0MDzj77bG8/739u9uzZg/POOw/19fVoa2vDl770JVQqlUljq9UqbrjhBhx22GFIJpNoa2vDGWec4f157fbbb8eyZcuQSqXQ0tKCj370o9i1a9ekMdlsFhs2bEB/fz+d++sZHR1Fc3MzEolEEItGo2htbUUqlXrTf/vTn/4UY2NjwbvgQIlEIpg5cyaGh4eDWC6XQyaTCZ7flpYWOOeQy+UAADfddBMqlQo+//nPv61zvpu89/5W8Tq2bNkCAJgyZUoQK5fLWLlyJZYvX45//dd/Df6sdNlll+F73/seLrnkElx++eXYtm0bbrrpJqxduxZPPvkkYrEYAOCf/umfcM011+Css87CWWedheeeew6nn346isXiW87nV7/6Fc455xxMmzYNV1xxBaZOnYr169fjZz/7Ga644gpcdtll6O7uxq9+9Sv6EfeNvPzyyzjxxBPR2NiIK6+8ErFYDLfccgtOPvlk/PrXv8axxx47afznP/95NDc346qrrsL27dtx/fXX43Of+xx++MMfBmPuu+8+XHLJJVi1alVNBcxNmzbhoosuwmc+8xlcfPHFWLVqFS644AI89NBDOO200+i/YR/hd+zYga985Stob28H8Nr9OPXUUyeNeeihh3DHHXcEY0ZHR/Hd734Xf/7nf45LL70UY2NjuPXWW7Fy5UqsWbMmKMb+oTz44IMol8v42Mc+VtP4d/q+3HbbbfjOd76DNWvW4Lvf/S4A4H3vex899759+3DKKaegXC7jy1/+Murq6vCd73yHvhhvu+02XHzxxVi5ciW++c1vIpvN4uabb8by5cuxdu1azJ49OxhbqVSwcuVKHHvssfjXf/1XPPLII7juuuswb948/NVf/VUw7pOf/CS+973v4cwzz8SnPvUplMtlPPHEE3j66adx1FFHAQC+9rWv4R//8R9x4YUX4lOf+hT6+vpw4403YsWKFVi7di0ymQwAYM2aNTjllFNw1VVXvWWd7eSTT8Y3v/lN/OM//iMuvvhihEIh3HnnnXj22Wdx9913v+m/veOOO5BKpXD++ee/6bjXMzExgVwuh5GRETzwwAN48MEHcdFFFwU/P/roo/G3f/u3uOuuu3Dcccfha1/7GubPn4/m5mb09fXh6quvxu233x68V95TvNsfVWph/0fjRx55xPX19bldu3a5H/zgB27KlCkulUq53bt3O+ecu/jiix0A9+Uvf3nSv3/iiSccAHfHHXdMij/00EOT4r29vS4ej7uzzz47+FjonHP/8A//4ABM+vPR/j9NPPbYY8651z5izpkzx3V1dbmhoaFJ53n9sd7sz0cA3FVXXRX8/3nnnefi8bjbsmVLEOvu7nYNDQ1uxYoV3vqceuqpk871xS9+0UUiETc8POyNXbVqFZ3D6+nq6nIA3D333BPERkZG3LRp09wRRxxhrsUbyeVybtmyZa6zs9Pt3buXjtm0aZNrampyp512miuXy86519a0UChMGjc0NOQ6Ojrc//gf/+Mt51/rn4+++MUvOgBu7dq1b3lM5/449+Xiiy+mfz56458tv/CFLzgAbvXq1UGst7fXNTU1TbrWsbExl8lk3KWXXjrpePv27XNNTU2T4vufm3/5l3+ZNPaII45wy5YtC/7/0UcfdQDc5Zdf7s1z//Vt377dRSIR97WvfW3Sz1966SUXjUYnxfffn9fveYvx8XF34YUXulAo5AA4AC6dTruf/OQnb/rvBgYGXDwedxdeeOFbnuP1XHbZZcF5wuGw+8hHPuIGBwcnjbn88suDMS0tLe7RRx91zjl36aWXujPOOOOAzvenxHsqKbzxv66uLvfQQw8F4/Zv7h07dkz695dffrlrampyvb29rq+vb9J/9fX17lOf+pRzzrk777zTAZh0TOdee+jeKik888wz5t+kX0+tSaFcLrt0Ok0382WXXebC4bAbGRmZtD533333pHH33nuvA+BeeOGFN52TRVdXl+vs7Jz0QnPOuf/5P/+nAxC84N8qKVxyySUuHo+73/3ud/Tn4+Pj7tBDD3WzZ892/f39dEylUnEDAwOur6/PnX322W7p0qVvOf9ak8InP/lJB8Bt3rz5LY/5x7ovtSaFBQsWuOOOO84b99d//deTrnX/OR599FFvz59++ulu/vz5k84NwPX29k465uWXX+6am5uD///sZz/rQqGQGxgYMNfnf//v/+1CoZDbtGmTd95DDjnEnXrqqea/fTNKpZL7yle+4i644AJ31113udtvv92tWLHC1dfXm/vKOeduueUWB8Ddf//9B3S+9evXu1/96lfu+9//vjv77LPdhz70Ibdv3z5v3I4dO9zq1avd2NiYc865tWvXukQi4davX++Gh4fdX/zFX7jOzk530kknuVdeeeXALvpd4j3156NvfetbWLBgAaLRKDo6OnDwwQd7yoxoNOopRDZt2oSRkZHgzxJvpLe3F8Brf+IAgIMOOmjSz9va2tDc3Pymc9v/p6xDDz209gt6E/r6+pDNZnHwwQd7PzvkkENQrVaxa9cuLF68OIjPmjVr0rj9c35j3eRAmD9//qS6BwAsWLAAALB9+3ZMnTr1Tf/9LbfcglWrVuGWW27BcccdR8dceuml2LJlC5566qlJfwoEgO9///u47rrrsGHDBpRKpSA+Z86ct3M5lMbGRgDA2NjYW459t+/Ljh07vD9PAfDms1/xsr/u9kb2X/N+9tcH3jjP189xy5Yt6OzsREtLizm/TZs2wTnnPUP7ebt/Tvnc5z6Hp59+Gs8991zwzF944YVYvHgxrrjiCqxevZr+uzvuuAMtLS0488wzD+h8CxcuxMKFCwEAH//4x3H66afj3HPPxerVqyc9D7NmzZp0fy+//HJ85jOfwcKFC/GXf/mX2LVrF+6//358//vfx7nnnosNGzb8ySsM/7Rn9waOOeaY4O+WFolEwksU1WoV7e3tpkb5jQ/De5VIJELj7l3quLpmzRpcccUV+NSnPmVqtW+44QbcdddduP32270awe23345PfOITOO+88/B3f/d3aG9vRyQSwTe+8Y0gCb8T7H/4X3rppXesTvF63o37sr/wfNttt9HE/cYXkzXHt3PeUCiEBx98kB6zvr7+gI9ZLBZx66234sorr5z0bMdiMZx55pm46aabUCwWPUn0zp078cQTT+DTn/70H/y3/Y985CO47LLL8Oqrr9JfCADghz/8IdavX48HHngAlUoFd999N375y1/iqKOOwuLFi/Hv//7vePrpp7F8+fI/aC5/bN5TSeHtMm/ePDzyyCM44YQT3lSp0NXVBeC133bmzp0bxPv6+t7yt7p58+YBANatW+cVUF/PG3/rtmhra0M6ncbGjRu9n23YsAHhcBgzZ86s6Vh/CJs3b4ZzbtK8X331VQCYVKh8I319ffjIRz6CpUuXBoqrN/LEE0/gS1/6Er7whS9QZciPf/xjzJ07F/fee++k81911VVv82o4Z555JiKRCG6//fa3LDa/2/elq6uL6t7fOJ/9+7G9vf1N9+OBMG/ePDz88MMYHBw0Py3MmzcPzjnMmTMn+ET5hzIwMIByuewpoQCgVCqhWq3Sn911111wzr1t1dHr2a8qslR72WwWf/d3f4evfvWryGQy6OnpQalUQmdnJwAglUqhubkZe/bs+YPn8sfmPS1JrZULL7wQlUoFX/3qV72flcvlQGp26qmnIhaL4cYbb5z0W9z111//luc48sgjMWfOHFx//fWTpGvA5N8I6+rqAMAb80YikQhOP/103H///ZOsC3p6enDnnXdi+fLl3p8AauFAJKkA0N3djfvuuy/4/9HRUfzHf/wHli5dav7pqFKp4KMf/SiKxSLuuece+qW2vXv34sILL8Ty5ctx7bXX0uPs/03z9eu3evVq/O53v6tp7rUyc+ZMXHrppfjlL3+JG2+80ft5tVrFddddh927d//R7kutnHXWWXj66aexZs2aINbX1+d9Cl65ciUaGxvx9a9/fdKf3V7/bw6UD3/4w3DO4eqrr/Z+tv8enX/++YhEIrj66qu9T0LOOQwMDAT/X6sktb29HZlMBvfdd98kFeD4+Dh++tOfYuHChfSXvTvvvBOzZs0yfzPv7+/Hhg0bkM1mg9j+PyW/nlKphP/4j/9AKpXCokWL6LG++c1vorm5GZdeeimA1xSR0WgUGzZsCM7V19f3ln9u/VPgv8UnhZNOOgmXXXYZvvGNb+D555/H6aefjlgshk2bNuFHP/oRbrjhBnzkIx8JtNnf+MY3cM455+Css87C2rVr8eCDD6K1tfVNzxEOh3HzzTfj3HPPxdKlS3HJJZdg2rRp2LBhA15++eXgm47Lli0D8NrfHleuXIlIJIKPfvSj9JjXXHMNfvWrX2H58uX467/+a0SjUdxyyy0oFAr4X//rf72ttThQSeqCBQvwyU9+Es888ww6Ojrwf//v/0VPTw9WrVpl/ptvf/vbePTRR/GZz3zG+05BR0cHTjvtNFx++eXo6+vDlVdeiR/84AeTxixZsgRLlizBOeecg3vvvRcf+tCHcPbZZ2Pbtm349re/jUWLFmF8fPxtXb/Fddddhy1btuDyyy/Hvffei3POOQfNzc3YuXMnfvSjH2HDhg3Bffpj3JdaufLKK3HbbbfhjDPOwBVXXBFIUru6uvDiiy8G4xobG3HzzTfjYx/7GI488kh89KMfRVtbG3bu3Imf//znOOGEE3DTTTcd0LlPOeUUfOxjH8P/+T//B5s2bcIZZ5yBarWKJ554Aqeccgo+97nPYd68ebjmmmvw93//99i+fTvOO+88NDQ0YNu2bbjvvvvw6U9/Gl/60pcA1C5J3f89ka985Ss47rjj8PGPfxyVSgW33nordu/ejdtvv937N+vWrcOLL76IL3/5y+an85tuuglXX301HnvsseD7M5dddhlGR0exYsUKTJ8+Hfv27cMdd9yBDRs24LrrrqN//tq5cyeuvfZa/PznPw9+kYlGo/izP/szfOELX8DOnTtx3333obOzE8cff/wBrfm7wrtS3j5ArG80vxFLwbGf73znO27ZsmUulUq5hoYGd9hhh7krr7zSdXd3B2MqlYq7+uqr3bRp01wqlXInn3yyW7dunacCsRQ3v/3tb91pp53mGhoaXF1dnVuyZIm78cYbg5+Xy2X3+c9/3rW1tQXyuv2AyPOee+45t3LlSldfX+/S6bQ75ZRT3FNPPVXT+rA5Hqgk9eyzz3YPP/ywW7JkiUskEm7hwoXuRz/60Zue56qrrqJqMQDupJNOcs45d9JJJ5lj9q9BtVp1X//6111XV5dLJBLuiCOOcD/72c/cxRdf7Lq6ut5y/gfyjWbnXrs33/3ud92JJ57ompqaXCwWc11dXe6SSy7x5Krv9H2pVX3knHMvvviiO+mkk1wymXTTp093X/3qV92tt95Kr/Wxxx5zK1eudE1NTS6ZTLp58+a5T3ziE+7ZZ599y3Pvv49vXKNrr73WLVy40MXjcdfW1ubOPPNM9/vf/37SuHvuucctX77c1dXVubq6Ordw4UL32c9+1m3cuNFbh1okqc45d8cdd7hjjjnGZTIZl0ql3LHHHut+/OMf07Ff/vKXHQD34osvmsfbf32vvw933XWXO/XUU11HR4eLRqOuubnZnXrqqW+qXrrgggvc+eef78V7enrcueee6xoaGtyRRx45ac3/lAk59y5VIcWfPLNnz8ahhx6Kn/3sZ+/2VN4Wjz/+OE455RRs27btTesfQoj/x3+LmoIQQojaUFIQQggRoKQghBAiQDUFIYQQAfqkIIQQIkBJQQghREDNX1778j/9LY03NU2h8WTCb4/pqv5X0QGgoYF/A3RgiPdfDkf8v3g1N3XQsVOaua/Rlq1+4xwA2PrqOhpfcoT/rcjOGbPISGDz9hdp/Kil3CN/9qz5Xqynbx8de+9PeR+GOXO5pcDsqV00/tJzT3uxpUecSMfOPcifHwC8utk/BgDs6+Vf5T/qqBVeLBLjXywqlgo03t3H78/Tz/7ai6Xi0+jYSoFbllSqZRofdzze3jzXi1364b+nY2Mx3qrU+tttlfxVt39okI7dsY37QA0P8W8KF4t8bcfG/W+557ITdGy5xPuLlIr+t6cBIJT1n/3maB0fa/5Bm/8gm8vSeCHm37dQI/dAKhT4mgwODdP44088ReNj5EuVkTDf46MjfG1Ljr8n2ZfwcqP82h34OUcG3/pLn/qkIIQQIkBJQQghRICSghBCiAAlBSGEEAFKCkIIIQJqVh8NZbnCIed8/3EAcAVfbVHI82p7MsVVCPFEksb7evd6sbmzufommeJ5r398N43nqlw98dijD3ux+kbeRSpG+gcAQPduvlaxkD++UuHqgWyOKzCK7Xz8C8+/ROOhkL/m4Rhf72ee5a0O02lfYQYAnZ2H0fi2nbu8WN/ITjo2EqryeISv7YlHXuzF6hr5vnr6+XtpfHQfV+v0EY99AJiV8VuvhsMH1sHMarlUJOqeoqHsqTfUe4VCnsYrFa6mChN1izOUMKUyn4tz/L7VkT1uqYwKhrKpYqgXy6TBDgBs3efvt3Kcn7S3l/eX6OsfoPG9e/meiMR9dVOmjj8ndUne8GtfL9+H4Yh/fyJRvt8ihtqtFvRJQQghRICSghBCiAAlBSGEEAFKCkIIIQKUFIQQQgTUrD6aOXURjeeLYzSeq/rKh1SMqySyOe7H4WJ8escfe5IXK5e40mLdBu7PUwCfdzTNVRXZvb7vTKbVyKlhru4YGOLnLBFF0dlnXkTH9uwdpvFlh3PFzy8f/jGNz5690Iu1t7XTse1t3FeqYHjobNu1lcZ37fE9kRKxBjq2bQr3rOru7qbx3ICvBukZ3E7Hbtjkq1IAIO64L066YSaNN2cyXixk+NxYWIqaElHURA01XijCFSihMN+flrVQqUy8gkL82IlEgsYLWf4couqrkiYMNeJOoi4EgP4R7v3UmOIqnnSdr8CZMHySqhP8ubf2RHOmlcYdiPrKUHAVS/w9Ua1wBRf/HZ7fTRfix64FfVIQQggRoKQghBAiQElBCCFEgJKCEEKIgJoLzW1TeLFxeIQXYmLO/6r6nt0b6dj6+iYaT4S4jUTU+YWlcpkX+HZt5V9TL1eN5hRW4a/gx8eH+DHiCV60Sjbw4tRBC/xGOG3tLXTsxo2baXzLjmdpfMnSZTT+vuNP9WL1dXy9WXMPAJjIcoFAsZKj8ZkzOr1YRxtvhGPZK6zfzLfs9t0bvNho7lU6NlTh9y2fNyxEjGLj9q3D/tgCt2iIG9Ynli1GJOqPr5JnCgDKRmGyZNhZWLYQzpH7HOK/N1ZJ4RgAqsaxq2V/fNwokM9tn0rjc1q5+MCy1qiSIuxElO/N9mSGxtfv4aKEnhG/IREADAz6FhXRCF/DQp7fz3Q9t8VoavSFOkNGEyBLfFAL+qQghBAiQElBCCFEgJKCEEKIACUFIYQQAUoKQgghAmpWH1XBLQ1K5VEa7yaWBuMT3OYhXsdz0+GLlvK5kK/jtzVNoWOPO963xACA519YQ+N5Y45TWvyv9bOmJABQrnAVy7HHnkDjVTL+lfXP0bEvvcQb3oTTM2j8mKNPpvESUeAUi1xlY1kaVIyGRI2Gimlqx3QvFovyLVg11EfdA7xp0PPrnvBiY/38GKP9fC9n2rldQmMrv/7eQb9RU4/VkGfWLBq3YPYXhQJXzljNdCKGAqViKIdYxx9ji6NiqIzKJb4nqkQdWKzw+2ApAKNRvj8t346xrL/Hxw2bi7KhYNq6jzfjGp3g6iMm1qoYxw4ZqiRr/DBTPBn3Z2o7t6ypBX1SEEIIEaCkIIQQIkBJQQghRICSghBCiAAlBSGEEAE1q4+efvY3NB6P8fJ3pepLAph/EABs2egrlQBgZOQhGm+f5nslVat8HqkEr8K3TuH+Km0H82Y1W7f6qhdLfWP53+zZvonGjz7qRC/22Lp1/Ng53pgkM4UrZ3b2rqfx1kyzF5vSxNekWOLntNQwuRxXlTRnfO+WcIg32ckVubqjkONxN0GUKVX+O0+mg6tYmpr5XBDlapAIaajT09NDx+bzhtcWPyOKRN0TjvDHNWH4KhXyXH0UNVRJzKOnWuIzDBEvIwBIlfmaR4lMxlLZOGNRqoYqyfJhclQhxQ9ueTaFDe8n68axsKXIKuZ53MZfw/q6Ojqyu5s3KqoFfVIQQggRoKQghBAiQElBCCFEgJKCEEKIACUFIYQQATWrj6ZO592ARge591E57CsCEvVGDgpxb5n8GPd06SVqg3yZK16mTucygeOXraTxVJQrULYR9VEkZCgzDGnC1q2v0PhAj68U6O3hHeOqIa6SyI5wJVCmjquvshP+8V11iI4dH+f3uLeHK4G2bd1J4+Hor7zYsccfScd27+bqie7NfTSeTvt7qGMG98OaKPJjxBJ8D2UHrU5YGS/2xH/+go4tZrmCK2ooimKJpBdr6eDqsIzRkSzdYHTSM/ZnmDye4Tzv3lY3zo/hilanNn/fVozOcJavEu0MB8AZcqW6pL8nkgmuPBsc410Ec1n+DqqWDP8osraRGD9npGy9PziJpL8nQkbnPhc6UGXT/0OfFIQQQgQoKQghhAhQUhBCCBGgpCCEECKg5kJzvsgLLulGXiQuV/zCZ6Kefx0/FOdFm0qOF2KSpDHL8DAvhm7buIvPr/hzfmzwQnOV1HNiMb58hX5esK3L8LUaH/PHh5O83FTl9WSMGs2BsJs3CenZ4zeDacrw6ymW+Ul7u3mhuVTiNh/xlL+Iz/7+UTp2YoLbQoQaeDE4GvGLlrEUX5O2Jl447t/NC58pxwu2w7t9S4tUmV/7zBbfVgQAEoaFBpz/vOX2bKVDn1/3PI1Hm/g55xw0j8bjpAhbzhvzMwqtpuUEKTSP53nxfcDYy8RVBAAwYhTxJ0jzoX7WqAbAkFFoHjVsZVIpv+gLABNkLiFWwQcQT/D3YblkFeD992Eyxe1tCjn+vq4FfVIQQggRoKQghBAiQElBCCFEgJKCEEKIACUFIYQQATWrj7at76fxcIx/zbri/K9ZNzTwSrnZx4IX+DGe9yvrmXrebAJRLlkY6uXXU8xzC4T22R3+oY3Kf4U0KwGAJLEuAIBw3FcVtBhKoH0DXD2RzXFrgAVzudVDe+scLzY2xtdkw0Zuz2Hdz4lRPhemEHNGk51okiszhvsN+4+8P5fiMFcqFYtGQ5UK30PFEb4nls3w98SiqV10bFuj3xgKANJpvoZRokyxVGA7+wdp/KntvsIMAAb6+X1un+ZfjytzJUzIaHjjXO1xy56iYDSlGZrgCqGeYa722zPg3zfr2GGj8ZDRuwujhlqpWPT3XNiwokga975qNDBizZQiRIn52rGN92EN6JOCEEKIACUFIYQQAUoKQgghApQUhBBCBCgpCCGECKhZfTQ+yr1oSlWuIEjX+5X1kSpXg4QNU5N4ik+vXPUVBMQqBgBQH+cVflfhioB4mseHicJhaIgrYUIpviZDhj9TlTQymZlpoWMTcd6wY3TEOLYh7arCX7DOjoPp2JFRru6YMJRaKPLrbwxnvNgUQ320efMeGq9G+Pj+QV9Rkyvw/Ta9axqNFwe5MqWzkSs5ZjQ3erGWej6/tKE8i0W5yioR932y4oYia6bR7KlrkN+3V4d4vK3Db9bjDC+jUoWv1aChEBognkDdo1xJ1zfOvY+YlxEATIzzc0bI2kaNFjYVQ2VVqfK4A1+XKFUDGc2BjHen1WSIeYqVjWZHkSh/j9WCPikIIYQIUFIQQggRoKQghBAiQElBCCFEgJKCEEKIgJrVR63TuKpi375hGmfeR6EKV8401mdoPBzhFf7hUV/J4BxXlGSrvkIEAOIJ3gXNau+UG/d9ZGIJrj6qVLhKorHJ8Dop+QqCdIx3+2pt5WoDF+IqiV37umm8a/rh/jEMLxpE+JrkuUgChQmuTHFEwXbq0mV0bHuS75XuNFestGX9yYyOGfchzf2Jxsq8y1YD2csAUCHqkYksP2ddjO+3ErhCaizrr1UJfMFDIR6fShSAALBjjK/hjj37vFj/Pu6f1Ds0TOOD4/zYbK2KZWtdjY1ldDBLGGubSPj7dizHVXpDeX4f8nnuNxVy/JlgHkox4lkEANEYjxtPISqkI1spy5VXUaMrZC3ok4IQQogAJQUhhBABSgpCCCEClBSEEEIEKCkIIYQIqLlE3dHu+6IAQCrFVSK0kZHjp5s3dxaNW5X/oUa/4j7Wx5UW+/ZwNcSYoR5IRfj1xNL+3MNJo3OU4aNS18CPPX2235WrPM6VGckk9zSZMW8GjTc0ZGh8ZNRXmris760CADOnLaDxMcP7CXGuiJja5iuqCoZvTdhQQoUMS5fFC97nxRIJrjzbtYcrskZe5UqbfuLbAwATI/7cQxWumFs8navjZnfy+Iadu73YmHHs5nquvmnNcMVgnaGwe3LTTi82OsE9z4pFrrKy9n6I3E/L46dq+C1Z8fYmfp2fP+c0L/bkho107F2PPkbjoRD3rKoY3dGYVZKl6qtU+FrFjc6NLuzfZ1cxnhMarQ19UhBCCBGgpCCEECJASUEIIUSAkoIQQoiAmgvNF3z4TBp/+tmnaXxkxG/kcfB83sRlWhsvNI+N+Y1TAKBQ9As023byImHbHH6Jr27ZReNWU5GGhN9oJZ1K07GjUV5AyuX4V+lbZ7Z7sYleXoTbsoMXPTONzTR+wdmfoXFX9o//1CM/pmOH+/haTWucQuMNCV6ETI36hfbREl+TcgMvni4/6sM03to824tFo/x3nilN22m8+xm/uAsAe0jRFwBiIX9vdTTx+9AzMEzjh3bNpvGDp/rCgW3GMZrq+T7MNPDmQO0TXFBQH/H3FjeFsIun1vPDiqpWMbRc5vOD0ZRmwRT/+QGApbNmerEXt/O9bLjEIOKs35v57KvOP1CZ2FMA9ss3YhSaY1H/magYghnZXAghhHhHUFIQQggRoKQghBAiQElBCCFEgJKCEEKIgJpL1IkEVzgceojfrAUAxvKDXqytZSod29rgqwQAYEpLC43HiTClta2Vjh3JjdF4ut5oppPleosCsYBoj3ClSayJzzvcxNUT+YI/l5nzp9Oxv3+6j8Z7dnH11c7uzTReHPWvc/M2bgHQNpWrjA6v76TxTGYujZfaiKLKaJzSGOEKlDqjYUm6zrcKyZW4xUl9M1c21dXxPbFgBr/OGUR9NbWBN0caG+f70BElHQA0J/05zljE7UbKjivVRgt8L09v4JYwc6f4a7h33GgyE+J72bidCMH3J6kath2RMPcyKRb5XCpFvlcmRvw1L+S5Mi5hNHUqV/kFjY9x+48oOY7VNMdSJVnKpnCUxS0VlHHoGtAnBSGEEAFKCkIIIQKUFIQQQgQoKQghhAhQUhBCCBFQs/poZIh77iSiXG1RifqKiFCZqwosq5ORMa4ImNrm+8JUJ3h+q5Z4k4xDDuK+MMUCVzhMFH0lx0g392ZqS3F1y3icr+HwiK9kaGvhyoT6ZqvpB1dDvPTSb2l8x4Z1XmzabK4Cm9/AGyzVVfial4zGLKzHUiLB1URtEa522/nyehofdOS+kT0IALEQn3dHF/fQ6e7hqqxIxd+4w/1cHfb7TVtpfGKMNxmaMcVX002v8gclEjeeKzI/ACga17+HenNxGYulEGL+PABQLPjHbiAKKwBYNp+rrDoz/F2zoJ3ft+Zp/r79wHFH0rG7B/l9e2j1GhqvhPjechVfDRSmHcfepBGO8YNiyb+fVWd4TRmqtlrQJwUhhBABSgpCCCEClBSEEEIEKCkIIYQIUFIQQggRULP6aLSHqyRmzpzG/0HVV4M0GkqlcJarchpiXIHy4ku+n8/WDdzjp2r4wlTiXFXR1JCh8SVLTvRikVm8a9jgTsNvaIh38KoLNXqx7KAfA4Cw46qpVJz72Qz3cIXUQfN9z6pUPVd7hXq5IitvdJjLl/j4cMrfbuUyV1NVs3xt+4d8Ty0AaJrjK6dCcb5/StUSjTcYKpYyuPooV/R9dEIhrjRpbeD3sy7K71s06quyika3s6TRYS5f4Pdn216utBkY8Z/xcIS/ImKGyRF/2nintkOnclXbpScfT+PpOJ9LNMwVbDHSNe3IuVzZ1P7n3K9s/abtNL6xex8/J1HTlQp8v6VTfO87Q1GUK/jqo5Dxe325LPWREEKIdwAlBSGEEAFKCkIIIQKUFIQQQgTUXGhe/fQGGh8d5kWRKvGuiMV4gwvwQ2D7lm4af2m7X7AtkiIMAISNgliqnhe9kxFeDA7n/e+epzK8UFTMG/YCE3x8mnQNSlV44Qslbg0wMswLnM2dGX7OhH/8kc28QF5f5gXOSoIXpkvGropH/etPhPnNf3nTyzQ+1sQbNcUG/XUpGAXIsONrVYFvnwIA1SQvEsdJ4bw+Ztg8bOb7qrfCi76xql+ybWrhRekCGQsAAxNcwPGfv/ctTgBgL9lb9cQqAgCqhhdDxSiSMhoNixPrN9VKnhdsTTFJiexbo/tMewtvmLV88SIa3zPMhTdV8r4JR/kDETcEAjDWsJD3i8flCi8oV8tWyf+t0ScFIYQQAUoKQgghApQUhBBCBCgpCCGECFBSEEIIEVCz+mjmnHk0vvz9p/F/QCroVeNr+uUsV+vs23o/jWcapnixtgXcosA5rpKwmoQ0G9X8yLivNpi2aDYdOz7OVSLzj15O42ufesyLVcaG6dj82BCNt0zx1wQAFi1eQuMxonzY+KQ/DwBozPBjh6J8bZNGk6HcmN+oaOf2nXx+U/j9PObkM2g8kvCVTdWqoYwzFCilpiZ+7PetoPFXHvmZFzuknat1Fi2aT+PNCW7FMY1YQIyAW3/s3sstF4YNq5CJKlf95Ar+nqj28YZJySR/dYRD/N5HyFYZHOR7eWRkmMbbmrhCyDGVEQBXJSet8t+DQ0TZAwArFvD79sjzL9H4roEBLxaJcZVegdhwALD6GiFM3ll1Sa5oLBt7vxb0SUEIIUSAkoIQQogAJQUhhBABSgpCCCEClBSEEEIE1Kw+WvvC72m8rpHnFVZA7+sbpmOntnDFxou7uV9MQ4uvTIlGeYXfioM0MQGAHb29NL7tRV9tcFI9r/AfvYw38iiUuXfLIUuO9GLr1z5Bx4bi/JwtbdzL6bAjjqXx5558yIu9umkLHbuxvInG69K84c8JJ/AmKW1TWr1Y6xTuZXTQ8afyY3TOonGmNMrluNfW8PAwjY+O9tD4+q1bafwHv1ntxQ5q40qtZbO6aLyjvoHGX+rxFUXbRniDoSHDE2hfjqvDerglEopkf4Yj/PmpFvg+TBhqmAj8523fGD/Gjm7uBxUHVza1NGVoPBrzX28h431QNfyGutr5/Tz/mKU0/u+PP+kfO8zvg+V9VDEUUhX49ycUMdRU/JQ1oU8KQgghApQUhBBCBCgpCCGECFBSEEIIEaCkIIQQIqBm9dFQH1c+lIZ9PxsAQMiv5u81VByFfq42KIzyY2dafEXA+NgIn4dBIsn9iYoT/DhVcp1bNnF11OYtu2h82dKDaXzmdN/TpdxgdLYyup0VCtxHZmCQd1N79snHvVh2gvvcjOe4N9W44SsV6+qk8dAU/zqjIe5B1d23ncbXrH2Gxl94yR+/Yxfv3NfXy/fbzp3baHx4sJ/GS0XfW2h8gp9zyz7fEwcA5nVw5d0E6ZzVP2F09DP8hipG57lEgqvGUmlfweYMVY7ZYc3odBgivmdjZT6/n//uBRr/8En8ddXYxJV3rKlfucw9jlyUX48zFEInH3YIjT+/21eNPbN1Bx1ruROVjU5yzCqpYhglRY1ub7WgTwpCCCEClBSEEEIEKCkIIYQIUFIQQggRoKQghBAioOYSdT7PK+L5Ej+Eq/i+M+3NvvcNAESIRwkAjGa5YiPa4+eyuNHdaLuheIrGjO5Tee6XUyj5qoXUPt7xqrGRqyGG+7mKJeJy/ljDJ6oxmaHx3gHu2/PAT+6k8Y0vbfBi1hpGS1wnMW3GTBpvaGik8ViMdPaqcPXExvXcb+kHP/41jfcPjnmxSoXPu2R4UOXzRkc2LlhB1fnKqfEif04qYX7sZ3bw+1Ymyq5IlPsKpep4PG4ogeKkSx0AhImPTsF4HhJx4xghfs5y1V/Ecog/93srvAPeA2t5l77n+4wOe6O+j1m64ndQBID21gyNT23j6rD2Rr7Hzz3iUC/26l7up9Y7xN9vFaNDZbrOV41Vjd/rCwW+x2tBnxSEEEIEKCkIIYQIUFIQQggRoKQghBAioOZCczrFC0v1KW4X0dTgW1EUW3jRyvgmOaadbxSmSeOPiNFsonz8Uho36psoFrmVQLnkF24ixlfJUyluOxBLGA2JqqQobxSUx8K8WD3Y6xerAeDVLa/wc5ZIQdRozGE18uicwRveRMO8CFckU9y1lxfbHnh4LY2zgjIAODL5qlGwi0QMu4QM329xyxKl4N+33ASfX71hxZA1GuRESRE2YRSILdFEOsXtLKJRbi3iSKOiaoUXzs29YhS3w8SjoWJYTuQMwcNAnDdkqjh+38Zzfjehym4uYJjZx61PTk7zJkjFZJrGW6ZO82KHH7aUjn32ed64bHCEW9YU8/67KWTsZd7mrDb0SUEIIUSAkoIQQogAJQUhhBABSgpCCCEClBSEEEIE1Kw+asj4aiIAKBqHiBP1Ubze+Dp6lVfKw/WGZUCZHMdQmsS4cAQxozhfZ6TJKFHgxIyv+kcM9UTIsAAIk3YbFWNNhquv0vj2rbxBTHGENyqiM7TWpI4rLbrmL6DxUoirYda/6s/x2Re5GmR43G9gA5i3GRWinIlG+DwspVqVHAMAolGuJkvXZbxYfUMLHRsnFh8AEEnyPU7tIozmK1ZDlVSS709Ll1IhSiNLfVSpcOVQOGTIkohKplziSr+wYX+RqudKoIlxbl0RbfRVSbF5x9Kxw9ufpXG2rwAgb1iobB/0FU+z5/HnZHErv56f/JpbuWwfHPaD1s18++IjfVIQQgjx/1BSEEIIEaCkIIQQIkBJQQghRICSghBCiICa1Ud1RuU/azQVMWU/hLAhKQmX+bFdhHm0cDWAM45txmkUcEQ9ETaankQN1UfS8I+KJ/y1ika5cmZGkc9wzWM/p3FLPVIm199Qz71yEob/y65+X2kBAL9+6Xc03jcw4sWKRe79U2beTACKxIMKACJh388nbPjwWN5HpRL3j7KOkyCeSFaTGUvZlEpyZRPzISqXuSIrneLqsJChBGLHBvheqVYN9ZGhHLL2G9u1ZaPZUUOymR/DeGbzOe43FY/7z1AswT2o+pK8mc6Dz/vNqACgI8P9vcaJUq2tKUPHfmA+9w7riq+g8R/9/kUv9vxu3ugrZ/hK1YI+KQghhAhQUhBCCBGgpCCEECJASUEIIUSAkoIQQoiAmtVHVten8bEsjZeIesTy/jHcUmDlLKYGCRm6IUvZZJ3UOa7MYEoO5hUD2AoZWGoQRzphJXl3LNNaxojHEtyHKU7UOskEVzxZ17nh96tpvL/CFR6hmL+HSkWuqMlnuZ9N3Ogyxm6opVYpGYqnYpF3BowbHldsb0UMpVLM8MOytme+5D9X0QjfE9a8LeXQmz1xtR6jZKiPiqy9HkDlR1bntYShpnKG91PVUDHlyBxZB0UASEzhSqCNG56m8bFB3qntyIXzvNjxXRk6Nm0844u6Omn8snpf7fazFzfSsf+5kXuh1YI+KQghhAhQUhBCCBGgpCCEECJASUEIIUSAkoIQQoiAmtVHMUP1MT7O/W+Yd00oxKUWlqLGUkkYGh7rIHz0O+CJZHnIuJLlO2JdqK/AccZYSz2RIj4vAFAqcGUKVbIYSxiPcZXE0hncE2mowv181vb6cx8cHaZjy4ZCKGr4FoVCZI7GclvdtCx/oqThT8S8e+x9xX//Kha4+ioa9idv+Y8VClzxUzL2ijVHpjLL5/mxozF+H8bGfH8rAIiw/UYUcNY8ACCf40rHsqFgY/c5l+U+Sea9b55G4+HcLhpPR/1ryk/wdySMedcZHmnTO/xulisP9dVOADAyYajAakCfFIQQQgQoKQghhAhQUhBCCBGgpCCEECKg5kJzKsWLbQP9gzTOCjekdgYAcEal2RrPhodgFA9Naw0etwq8LHxgpW27wGc1G2FEotwuIRrh8WzZKqr6FgBpo/BXDRt2Ccb1HNLOm/XEo37x64dbBg7k0KZVChteKvFCnnVsq/lOPMltF5hNQ8VoDBUxCrPVChcl1NX5ViFWs5+yYTlhrVXYEHwwi5dw5MAaFVkFW2aLETOsc4pGAda6/tHRoZrHW/YpI0P9NG7ZxxTCXMCxY2+vF0saazVvRgc/uGG1wwrn9UkuMDm4gzcqqgV9UhBCCBGgpCCEECJASUEIIUSAkoIQQogAJQUhhBABNauP2lr8Bg8AMNTLv05dH/cVNSGjSUjIUBVETPWEH6uaihIet7/qzyv/YXJS1njntflxRUnUUPE4ELsE8LH1Ua7MsBQLuSI/TjhMLADK/F62NBnNdwxl06Y9PTS+euMOL5Y1LADCcd6ox7YKYQ1v+PbO5/k5Y1GrgQ/fK0xpY+3ximF9Eo9zVV+MNPapVLhKrWrq4Ix5G3vFeiYYYUOp5oxzsqilVCoath3lMldZ5bKjfDyx+chPcPXR6DBXH1nNhMaN90pXxn9Ptjbyc2ZGuUovFuV7PE0aZrU0ceuTedPa+QRrQJ8UhBBCBCgpCCGECFBSEEIIEaCkIIQQIkBJQQghREDN6qNlHbzCX9rNm1YkB1/1Ys2t3I8jWpfhcXAVQlO9X4WPh7nqIey4Wqdc5GqQkhEPEbVF1XGVQM5Q1ORzhrcQUaaUC4ZSi0aBstFMJxzl6pYi8dwhgiQAQCrOfZUGx7hv0ZpXt9P4y3v8vRKv4yqJmNnsyFLIEHWYoWqzmrikUvw6rfHM5ygUMhbRIJ7k/j+885TVdIpfp6UyssyfmPooZJwzbCgDq5YMkBy7ZHgcWc10wobar5jj74l83j+O2ajHUBlZiqfxEl/bHcQLbtqUDB07ZYLPpaWJK++SZM0b0lwVevj82TReC/qkIIQQIkBJQQghRICSghBCiAAlBSGEEAFKCkIIIQJqVh899+wLNF6t8Op3rGm6FytEeAeriRGu+Nm5dQuNDw77nZbCFV7Jj1QMVQHxRQGAkhGnCpQIV/ak6zM03tY5h8c7pvqHTvJ8vW+Aq8DWbPc7PgHABR88g8bHxn2FVDzBvX+yY1xh1mYoHMo7eTe+5e9b7MXWvLidjrV8cSz1VYh4U1UNHyurI5vp52MpaogSqky6sQFAXZorSix/Jq6y4vOIGN3RLG8uaxVd1Z+71WHNUnYdiAsT61wHAMODfTTO/McAoGB0UwsTHypL8WR1P7TuvdW9LpfzVYDb9/FnM2k8b6kUV6TVN5BufFE+ton4JNWKPikIIYQIUFIQQggRoKQghBAiQElBCCFEQM2F5kMPnk/jBePr3mP9m73YcCVDx8aSvAAda55J46lkqxfbsn07HTtvWhs/hlFUjMaNJSENWHb38wJsU2uGx1t4PFfxi1ZFYqEAAHnH8/i8ubNpfI4R37RtpxfraPfXFQB27txF463TptF4fRO3M+kd9AuCB9LYBXiT4ik5TNU4dsS49+Go0SCnyu8FIxblBT7r2Fbl3JFCu1V855YYduHcIhLz5x4xXhGsiPvaVPhcqlU/bt17y4qiWDDsY7L8OUzX+Q1onGH9EY0ZjaSMxkbFAhewdI/4c6wY920iz4veI+O8cD5GitgHdfJ35KzWDhqvBX1SEEIIEaCkIIQQIkBJQQghRICSghBCiAAlBSGEEAEhV6P84xt/9QEanyhwZUasocmLHX7U++jYZMxQAhkNYibyviJg655uOvaQZq5scqlGfk7r2+FEPbKpjysTWtNcgdHV7q8JAGzZ46sNhsd40xwY9gJ5Q8mQafIVGACQLzL1BJ93qcTvcVOGr+GE0Tzkngce92J79vbTsbEY//p+Is5tVZjqJV/gzVeqzmiyQ9QqAOAMuwzmdBBP8HlHYlzFY6leuPrIavbD92F2gqtynGl/4VMlzZgAU/CEaIQ/QKwRUIXYagBAwbhvY2O+vQ0AjA9zGwkmkEo3crsRhPg7qFw0rFLo8wNUia1O1OhelTLee5kUf+9NbfLfH62NGTq2tYnHb7j/FzT+evRJQQghRICSghBCiAAlBSGEEAFKCkIIIQKUFIQQQgTU7H20bpDLDWbN9JvpAMCRS/14cx1XQ6QMpUldHY+Hq/6050zxG9UAQNVoqFJtaqHxYoyrW8YnfIVQYpQ3vOnJ7aXxgZ2+3xAA9O9iuZnfmnQdV1NZXjQjxhyZ6KxiqGyiUT6XkZERGs8SjxYAmMj6qhJL+xaPcxVLKsXVOvmCr8Apl7lCJGEohGA2VOG/O7GmLxVLrWM0wrG60jA1VdhQyDhD3RI2GvjAuJ4o8T6qGsqmstG8yvKJihH1VTTE51cy9rK1VxJJrtYJh/xnP1bP32PZEe43VMjy60+l6vg5ybNSMfZh0bigrCGBXLtjNzn2djrWWqsbeHgS+qQghBAiQElBCCFEgJKCEEKIACUFIYQQAUoKQgghAmpWHy1aciSNT01ztUWaiBAS4FX1eNRQgxg+PyGiTogYfjaVKPfnGStwFUJ/D/dX2bFznx/b3UPHds7nHjr7du+h8XW/2eTFQoYPUdTw0HGWb1HZUMOQta0a6psYUaW8dhCj+5blf0PGN7dwP6jZXbyr29FHLaLxVzf53lfrXva7/wGAI13AXvuBsd/MbmL+nqtUjC5thiqnXOQ+P2Vy38yOccb84oZqjN17AAhHSdx4NqMVPpdS3vCbKvmKtEScq4ZKzFQKtkdapcoVaR0N/vHLRX4fIkZTu7okP2ciyp+rJFGZJeKG/5ixV6zHrZr0f1Ax7kPR8CurBX1SEEIIEaCkIIQQIkBJQQghRICSghBCiAAlBSGEEAE1q4/yBV5t/9FTr9B4fb1fcc8YnbqSKa5wsFQv4Yg/7aLRCSmb495H41nuzzM+zruGjQz7qqSDZnPfp3mxWTSeTrfSeH2z7yFUMVRDFcPLqZjl3i2lIh9f39Tsz8/wj4rXcYVQ1PAQihnx2XE/Xi4bsg/w+CubuH9UGP79X3Awv55yxeh25riSI2J0CKuM+R2/xga4Is0ZJkeswxoAhCL++CjZ94DdkS0S5+e0uqaFiJeTNdjyW3JRHq+U/TlGLD+oOD9GOWWsYcVQSBEVU9hQNrlmvidgPIfWb9PMD6tsqPRKhmoqb3R7yxM12USZP99xdi9rRJ8UhBBCBCgpCCGECFBSEEIIEaCkIIQQIqDmQnMiypvPRIyGHdt2+JYOyX7elCWW4AWXmPE1/VjMH88KPABQKBiFWaMwXTC+pl8p+PHOqbzQXB/toHHXxAtic/7sCC/WbBTld+/cQeNrn/o1jQ/28oY/hx59vBfb2subIPE2I0A6VU/jVaN4Oj4x4cWsZiBWMZTZPwDAyIAvBCjmuWig0RA2OKMg2NTMGzLNOHihFxvbyYu+e3dtpfGI1VCGFKZdhS+Wtd4ho7ht2WIkiI1G2LBPMRxBUDasUkhfLESjVjGUxyvG/akatiWsaZQzRANxY02icf4OihjvmyixEDFumyGlAIoxfuyGpC/4GJrg77fhCS6kqQV9UhBCCBGgpCCEECJASUEIIUSAkoIQQogAJQUhhBABNauPEOKWEy0tvl0CAOzr8S0ARocG6NhEiiub4oZdQoKqj3h+yxHVEGCrjEo5rliZ1uErisYn+NjNW7nSxBl2BJ2dvoppRkOGjq1r4AouGCqwbJ6rrH6/erUXm7NoCR3b1NpO4+k63jykWOJ6pVTaVysVDBVYLusrlQAgm+X3rSFd58Xq69J0bGGMr2EqztUtbe38+ieG+rzY0IAfA4CqIUEJGc2hmPzKOUNlZEi4rPFlQ5XEBDimnYNhuVEyJDWOqJKMLQtjeihZ6iuiMnrtB77SyFqTgrGGFUOVlDTURyGyLgnjGLGo0cAnztWYiZjfNChGYgCwa5A3C6sFfVIQQggRoKQghBAiQElBCCFEgJKCEEKIACUFIYQQAbV7HxkV8QbSTAcAFi5Y4MXWb9hAx44O9tO41awlyjyRDMVCpczVLcUC9wZpn8LVVIsWLfJizRnefCZuNAcqWaqcpK8gsMZWypYTEcfyaBnq8b2pdhm+Sojx+9DZyZvYzJ3RSeNLD5nnxaYY6rV0m68mAoDhLF/bHFFZLVsyh45NFrj66KXVL9D4lARXCO2KDXuxbANXgwyH+PXkCnx/Mo+nmOHDA8NvqGIobSyvpErFv86SMTZinDNkNeUhfkZVQ/FjOSJZCkNrvCPuQkwF9dpc+DEMbRgqjp+VrVfUeB9Y74k6Q42ZJu/DirGGTWm+D2tBnxSEEEIEKCkIIYQIUFIQQggRoKQghBAiQElBCCFEQM3qoxzpmgUAYcO7pS3jqy36G7iCaWJwH42XC+M0HiJKKKtDVKjK1TrxMu9YVBnhqqTR3t1erNXojhaP8lwbNiQOiarvoVQa2EXHZhxXzpy4mCuB3Dzu/zM+5t/PlvY2OraunquPWuu5wiHXzz2uUjG/C1xnfJSOLQ/zY2TzlneNr+SYO8xVbSMDfC8vSfLxzeAeNQdN8a9/6cmL6dhkM98rWUNltm6jf/9f3MDVYT2DvGOepbQJhwypDdm2YcOgyOqMF4nwH1TJ8xmNGM+J2XmNn7NqqJKYb5HlP1Y1OvoxRRYAlMPG2pLtaSmVQqTTHWAruJin2LDhETaS5b5staBPCkIIIQKUFIQQQgQoKQghhAhQUhBCCBFQc6F5aHiQxktFXrDdu9cvlI0O9NCxmRSfhmXpEAn51gBhw+eiCl5AiiZ5PmxpML5iXvYLvC1umI6d3jCFxuc083PGSn6hfXD3Tjq2PcO/Gl9/MLfcCJV4oTlEvo4fqhiWCwW+hqWi8TtFMy9YhxuJ1YNRlC+M8HvfGOVCgEiEiBh2D9OxDTm+Z+vzxj40nEViFb9IPnd6ho6NxoxGOK28AD2r6tvHnNAyi47dM8HX5NVd/JntL/JC5pZ9fsE6X+AXH7KazBhF4ggrBhuFY8uew6puG2VzWsitGkXfqlF8rxKrDAAoWo19SGE+agwtlA0LkSIvbueK/n3eOzRszM8y6Hhr9ElBCCFEgJKCEEKIACUFIYQQAUoKQgghApQUhBBCBNSsPgobXw+fGOqj8QHSxMUZMg6jT4T5dW8mNwhF+KU0pbn65vQTj6Tx9x19OI23xH2lwO4t2+nY3c+9TOMdGX49HRlfaTJ9lCtKCpv419pHs8YikuYzABCLk6Ynhhoia8hEyl28ic1AXT2NJ8b9e5RIcOuTSqWFxhsK3NKhEf7eSnZw649yie/ZYoVbiITr+fWER/316n7iJTr2uSbjegz1UUfMP3aD8aDM7Rmm8UNm8nOWOzM0vmmvv7bP7eL7bUsPtycZHuf2CuWKP/eIofgJmUpCjvWeoK8EY6zRSwjhKLeiMMRXIJeJfJm/OytZroIbN57ZgXH/ndA7yu9PxbqgGtAnBSGEEAFKCkIIIQKUFIQQQgQoKQghhAhQUhBCCBFQs/ooHja8NMq8Us6EBSGjYYclP4oYiiJW+a9Lcc+izrYMjXfUcw+hgR1baTza7B+/NcGVFk3TuaLEvcj9jAbGSZMhY7kjUT7vaIo3wgmnuLpnJO6vbbjJ8E9K8LUdrnD1RCHL1T3phH/juie4Iq0vy9VXC9tbabyuyfdbGjIEWQPj3BPIRbhap3eIK23iSV+Zkp7TQce2lYzmQBN8D/14fa8X64jx52Gl0fClqcc/BgCEe7liZeGCaV7MaiT1aNloBNPBr38f8cna3d3Nj1Hg995SGTnHvblC1EPJGssfOOu35qTxHEYiLG6o9wxV0jBRGQHAvhH/vo3n+fNTtfyjakCfFIQQQgQoKQghhAhQUhBCCBGgpCCEECJASUEIIURAzeqjQpFXuQtlXs0vk4q7I52QXoNXyi1/kdmd7V7svPcfQ8fOauDzy6S5eqBvF1dsFLb4Xk7jhh/UrzdyD50pg+M0fspiXznTtGA2HTuU4GqQUaM72GAf9wp6YM0WL3bwXK5gOqqFdEwD0D3IT7rB6Hi2qODf59Xb+dimDD9n21T/3gNAW4vvH9XUyD2Lntu8g8bHBvj9GR4z1Edpf736Jrgab8jwuTm8iyu+kk2+4qtlCu/oV7d4AY3DUPeU9/E97tb7+zbczF8RJy3k96F5OleHDdX7c9xZ4Kq2Nb/9DY3v3rqRxitlvrauQp59d2BeRhFD8RSL8N+no1Ff7cf8kABgNMv3ykQ5R+PZkv++yZf4+61iGcrVgD4pCCGECFBSEEIIEaCkIIQQIkBJQQghRICSghBCiICa1UeJZIL/wKjaR2N+lb9qnC5UNbxb6rg64c8+8D4vduhc3mWr1Me9jLIvv0jjsd3cF2Zwp6/iKRL1CQAsMfx5pnZxb514ys/N3QNcgfDMIPdF2bJ3mMZTCb62Oecrh17ZvYuOHRjhio3WOn796ThXdhWr/v2fMyNDx5YNL5q+Hb5qCgBCo77fUleKz7tnPVextHT43j8A8OFjZtN4ut5XjeVLfL0HBoZoPNLMFVJDOV8JtLuXq6M257jCbG6rcX8c70ZYKvoPc8OEoWIp8XOG0vyFUD/+tBc7snMhHbvoLy+g8ee3EI8wAI/97Mc0Psi6PxqXEzZeZOGw9Xuz4f1U8pVQE0WuEJrIcfVR2FBjpmP+XIolPo9SUd5HQggh3gGUFIQQQgQoKQghhAhQUhBCCBFQc6G5WOGFv7pG/jX9Kvl6eCHPi6TRGG8Ec9LSg2m8IeIXSYf28gY2dQlebCsN8blkrK+1E6uH1JJFdGy+mdsRjBrNWvaRYtYewxLj2Vd30/jwOB+fNgQCqZh/nYc08e0ws62ZxhtIox4AKBZ545j+sF/8ajOsT7Ihft+6h7mFSMeYX4TMGY2hlnfy4m7zPC4EaG/gaziR9+9nxCjMzmn3i9IAkDQaGxXDvlihJbqNjq1r5IX9dJNv/QEAjVP59VT7/evJbvCLtQDQv6mfxkN7uA1LqMsvbocmeLOjpjAvYh9/1Hk03jlrDo3//I5/92I7NnKBSQT8viWMQrMzhBA50vSmXOVF3yQpHANAJMzvZyziP7MJo/HSiNGkqhb0SUEIIUSAkoIQQogAJQUhhBABSgpCCCEClBSEEEIEhJyrrRvDZz/xERrP5rkdQy7vf907FOI56ITlJ/LJDfBmKKGsbxmwoIs3/UjnuBIm8sLzNM6aZABAuM5XiURmzaNjx8DVA1WjIVEo6SttJka5Ugkh/tX4/BRu8xE2GpA0EYVQZ4nfy1RzB40Xjevs7xug8VGiYGubylU55ZihPhobpvF1r7zixaZU+LUvMGxIYmne2Cee4Ne5a8hfr8VTuVVGfZxfT8S0BPFVLPkS38t1M/j1DBqWG80probJVHzVz1A3VwhtH+bqm1APVw6lJvxntuHwGXTsaIjPO9zUReNtKy6i8aGsf50//PZ1dOz2jS/ROBH8AACiRr+wYtk/Z9FohFNmTYAARI2OP1ViB8TOBwB50pAHAB5+jr9TX48+KQghhAhQUhBCCBGgpCCEECJASUEIIUSAkoIQQoiAmr2P+ge510nZqH5XiN/HnIO4V9B0w1tn59ZnaLyj2fdReeYl3kynocLVOoO9NIydBV/1AQBnHOKrkqYaKpvUFK6EGjaaakwQP5+uKB/b0cobpFTTXGlTSXJFTbLJV8mEu7m3Tmi4h8bDE3yOU4t8WzURJZhr4I2U4o1c3jHTaLxU6PD9pnbu4n5Y92/gzYTGClwNsnJBhsYXEuVUZYzviXwjV3CNDHEvp03d/hzHDBVLM1FeAcCvt/DGPnPmzqbxw6b7sa4W7p80M8e9dYZ3+82OACBb9PdnZRsfW+ng52xt489mKs+b78TaDvFiKy+4mI6968av0fj4CL+fRh8coErukeGTZKmMwob/GrMJC4f4cxIyjlEL+qQghBAiQElBCCFEgJKCEEKIACUFIYQQAUoKQgghAmpWH5UM3x5H/Dhew883iw47nI4cG+TqlsY0n97gqK9iGR2doGNjMcOjJc2r9ofP4iqR3jHf0+WJrdwXpqFhmMZzJa7WGRnz537uCVyp1T5rLo0X+/hc9uzqpvHtue1erH+Uq1UWd3J/ovnTuBdNodfwPhryJV/pLB/bMC1D400prkzpaJjvxZYZCqbefYaSDnx8cxvvJlYZ8o8zPsD9o4br+f2pRPnvZaV+/5l4fA9XKkUSXJE1r5WvVSrCn+X7nt3rxY4+mu/DkzK8e12sylVwIeKR1mD484Qz3Ccq2cCvB+NcSlgc9v3DZs48jI6dfTCPv/C7/6TxMCzlkB+LWd3baBSoOkPaRP5BxFAwJWOGOVMN6JOCEEKIACUFIYQQAUoKQgghApQUhBBCBNRcaLa+ql2t8HJJhBQ6prQa9g9b99B4Mc8Ls3Nm+sXgQ+fy5ialCd6sJrtxM41PaebX80Lej+/s50XSDtIgBQDOPowXsduL/lplOnnjlLFxXlAvGQX/vnFe+Bwnhfk5LbxRT1OaF5pLEW5PUqjn26rY7l9nscjvT2SAr22mjV9ntOKveTzNC5PTOoxtn+QWIlljH27Lk6Y04/x6coPccqNlJr/Phx3jF86PJM2YAGDHCG++M863ISLEcgIADmrzi8fTHN8/UWMuySVcfJDb6wserH0SMzwkSta7JsevJ5nwY8USL9Z3zJxD48Xf8PdepcoLvJZ1BcPubsZ/V4+yjj/G6Yw+RTWhTwpCCCEClBSEEEIEKCkIIYQIUFIQQggRoKQghBAioGb1UbViNNMx7C+YUmBigqskBsa4eqA0xJUcna0ZLzY8Pk7HNjcZX8dv9JuyAEB0yFeUAEAXUTHNiHF5x4wW/hXz1gRXMiSHfIVHcusGOraY5gqZ/AhvetIZJhIMAAsXHOzFKlXj6/hlLmUoZbkyJRz3GxIBhk2BM5qExLm6BUZzl2rRv/+hcX4vKyV+PcODfL+l2rmybcoUX0322NZhOnaoyq+nvI5biyyv+tYaK5ZxZc/hrXy9wwl+P0eN53Baj2+t0WrYwYSN5kj5ED9ndtRXk20d4M/9+ARfk0ZikwIAC7r4a2z+ooV+0LCcqBS5wiwR5fKeiCHvYYe3muZYFA2VFVMaVUkzMwAIHYAK6o3ok4IQQogAJQUhhBABSgpCCCEClBSEEEIEKCkIIYQIqFl9ZDWxCRlqg2LZV6Y8/9wzdGx7I1cy9I1xdcuOHbu92NGH+14xABAx/G+wmKuSyqNcsTKw2feuMWxu8MR2foz6Oq5Aed8sX90yUOKKkkokQ+Op6U00bt2fQtlXToXDXB0VTXAFSiTOx6cyGT4eMS/mqsZ1NnO/pXzW2Idrt3qxhOHxkzB8ouozXJEWNu5bpOAf/+iDZ9Kx60f4Xi4UuZpswezp/jwq/voBQCVreOWE+LHro/x6Qk3+vcj2cl+yLa9y77AU8+cBkEj6z3hlnKug4mne1Khn3z4jzpt0NU3t9OdnvGt2bnqFxmPG9VjangiRH8UiVsMbrhyqM1RjjIqxlw9Q8DQJfVIQQggRoKQghBAiQElBCCFEgJKCEEKIACUFIYQQATWrj5Yv9tUQADAwylUVfSO+SmTD82vo2NaTT6XxzLRZNB6P+6qPouFdEgVXq0QTXIUQnsIVKKmCr7S5bMk8OjZX4J5I23Zw9cT96/d6sU39/Hq6Onj3tgsO972MAKDOUAI1NrZ4sdAYV3GEC1xNFTd+p4jFuUomTzQbO3sG6dif/+YlGt/VyztnHdHq38+jDp1NxxYnuE/W7Ll8j8cN/6j6tK8emT6dd1JbZnTA65nge6Uh6qtKYhV+jEqEK7h27Rmm8e49fB+2Jf3ryQ720bElxz2oWqby/dk41Vf7HZ7hr58p87nHkzM6AA6M8bmE6/zOgOueX0vHvvrKC/ycZa4QioYNP7Cy/56IJbkUqGz4FkUM36J41FcxxQ1hk+WJVAv6pCCEECJASUEIIUSAkoIQQogAJQUhhBABSgpCCCECalYfLZ3tV/IBoL5+Bo0XSNX+6XU76NjHH/1PGl9x8vtpPFfs92JDFV6Gb8xzf5V0Y4bGu3v8YwNAtOgrP5oa+DGc455Ao46rWDZkfbVBznFVSksLVyYMRLgqZ9zxznjj5DLL/byzVUuZmzwlDQeY6ggfz7py9fTycxb3+YosAGg1vF7CEX9tx4uGQqaez3vj+k00ng7x+zYr5T8+lgouzg+BrhhXahWJV1LfDn49u1Lc32tDL+9gNlLm5zxxnq+8azqIezmN9vP9FmniCqFo81T/2FP5PCJxfn8a2rgyMDKF+35tH/b3/uP3fI+OjVe5sitkqXuM5mgsPJrje6JoHIRfDZCI+s+Po2cEilIfCSGEeCdQUhBCCBGgpCCEECJASUEIIURAzYXmRJx/lT6b50WUJ1/yi8q/W7edjh0c4cXgn9zP7QjOOueDXiwV5kWrbJ4XLCMThmVAgcerFb94PDLOC3/7BnihdXR4mMYP76zzYgtn8q/6L5jHC/sWu3Zz64reQd+6orWFl7hSrdy6Ye3aV2l8z47tNJ4k9hdNMV4Qe98CXrDMkK/6A0Cm0W8okwjz/RM1ivhWsbE4zufYG/LFF2HHC3+dxv3MV3hRdcPQLi/Wn+AF5VlzuR3McbP84i4AtHf6zWcAIDvqr9fzL2/h55zBC9BtrVyQUiz6ax4nTXAAIDOL28cMT/D7sOZZ3iDnqYfu8WITw1zYUBfnvx9XjWY1hRK/z875/6BiWGVYjBf5+FLFt/eJRozf643mWrWgTwpCCCEClBSEEEIEKCkIIYQIUFIQQggRoKQghBAioGb1UfcAb7Ty4BpuDbB+h1/lj4R4xd4QlGBsoJvG77xtlRc76tgT6NhjjzqCH3uUK4RyQ7zyXxf1l6rax9UqMaMpy9GLuQKljXxNv5jzlQYAkEjx5kCVCp/3jFlcJTJ7rr/oja1c8bNzx04ajzY00vhRxjlj5FeQkGGV0dTgK7IAoKkxSePxpL/mVfA1qRrWAK6Dx0f37KHxXNa//6NjfE/ES9xuZCzLlVAPb/Ab4WTBFYB/1pGh8VlTuWqsZxu/n42N/prP7fSbMQFA14K5ND61iyuhqiFfeTZqNBh68rfP0vhTq5+j8a2vvkzjcdIIqDHF17BQ4tY0JaImAgBn+V+E/PHO2IfRMP+d3Dk+vkC2UIS8lwDbhqMW9ElBCCFEgJKCEEKIACUFIYQQAUoKQgghApQUhBBCBIScM8xa3sDRh3DPndEsV8nEI34VPmYU7MOkYv9anE+NKW3G81w9kGzg6okjjj6exg87dDGNp0P+dVZHuI9KuMCVTfVG85DGOl8R0VDP1TcNDfU03t/D52J5Hx002/edmTV7Oh07Os69qSbGJ2i8zfBQqpZ8n6xojKtBUsb1T4xzP6Nq2T/2FEN9Ezb8YoolrvoY6ub+WYyxLN+Hs+bPpnFnNOXpH/Pj2SI/dkuKP1its7gSyPLiCRG/nGyW3+My8QIDgFH+OsDmHv++Pfv7tXTs1o3cy6ha5PvQUjUmSVOaOIkBgCH4QdFQH1WMeIiqjwyMRjhhU5VEjmSMrRiv9R/8hvuVTTrkW44QQgjx3wYlBSGEEAFKCkIIIQKUFIQQQgQoKQghhAioWX20cNYUGk/GufdGgkiNjMI/Iob6yPIAYVOuGGYfRUNpMcGMRAAgyr2FuuYu8GKLlxxOx86Y1kHjMXDFRnli0IvFK9xDJx0xZBIl3gUuZPiudE7159jazDt7pZJcIRQzlEOVHPfJCjn/+hP1XE1VyHKlyebtXAnUP+IrvuZ08vswYzrfy4U89+IZHTZUVsTjqVLixygWeHygz7/3AODCRL1nPGv5Mn+wIgm+l52hWMlWfX+i7iF+H3bs3E3jmzdtoPHBfX4nuYjjz6D9TuHxiKEmY2+VapnfB6a8AoCy4XEUjRhzYVoj8/1GwygaPkzsnWqpjErG+/AHT0h9JIQQ4gBQUhBCCBGgpCCEECJASUEIIUSAkoIQQoiAmjuvWR2L4kbbNBYPmS4ghqIG/NiOVNbDhq9Qk+G3NN1QGVmV//4dL3mxn7zCvVuiSa6omdrJO5LNnX+QF5s9ezY/RjvvjlaXMG5lmauS+kg8l+eLlchzlUgixuMVw/+mXPD9fFxfHx2bNTyOdu0boPH+EV/xNFLge2ICvDOeq/LrKRqeQ72bfSWUIXbDWMHY+8Y+dDG/w1wkkqZjs4ZnU89u7oe1exfvvNbTvcOLjQ9y7yxnqN3iEX6dqZivbAqH+Z61OpIRQdZr5yTHBoBK1b9vIfCxYePgpRK/nrzRSS9BlFAJ1nIQQCTCnzerMyDI+KihbDJehzWhTwpCCCEClBSEEEIEKCkIIYQIUFIQQggRULPNxYrDeJOduPV1b1JwsSwXQoadhdGDArRthdFowypiR40COWuSAQC5gl+0yhsFSMtawypOsWOXjSYe8YRfgASAxkwzjU9p5YXp1tb2msdOmcJtITJNvJlOXZoXROMJv8AbNYptlYpR9CWNegCgVGb3gu+JhGHPYRUbCwVeOS+QwvnIKG+w1N/Hi779vTw+NODHx0e4JUY+x4vyjjQeAoCkIb5oSPvPcsqyljAKmVHDcqJEngnrWUskDFGLYX9RJk23AG5zEbJed0bcaqbjjLmz608ahXBLeGNZ9jArjqoxNm7ctxt//hyNvx59UhBCCBGgpCCEECJASUEIIUSAkoIQQogAJQUhhBABNdtcpEiDBwAIG19Jd6SybogEEDIq/FXSlAUAoqRbjzWPasVQQxR51d5q7MOUEqk4VxXEo/zYdQlDhUBcMSqG9IqtKwAkQiM0XuzlipWtu172YmtzhuLH6EfE9R12w5IwURpZX/WPGfvNUqw4socsZUbVWFtL8cSaAwFAmCjbooYKztqfIUPxVJf0H826BF+TVIIfI1LH7TyShkKINcGy1spyUbAUNaxBjqVUspoJJQ1VUtHaoGQqlurQcpYwXk2mtUaUyLISUX49Rashk6FSDJGGP5GwZZXx9tEnBSGEEAFKCkIIIQKUFIQQQgQoKQghhAhQUhBCCBFQs/rIrGdblkNMJWL4i9RovxRQIT4qVUvdYRzaVE0Zc2GNL6yxlgLDEM7Q5iGWuoMprwAgHrW8krhioxD3VQsNaa5WsdRhllKrfACKlbTRDSRhxKuWRw05pyVKKZT5sStloxnKATR9sRrBhCyViLG2YaJiiYaNzWztK2MyMWMPsS1UsTatufcNJRTb45aqy1iTUNXa+zxerpA5Wkto3ThLwcaODcCRvRI3VIqNaf5s5nLca2si76uVShW+hmwetaJPCkIIIQKUFIQQQgQoKQghhAhQUhBCCBGgpCCEECKgZvVR3Gi1VDKq85Vy7Yoiy1/FVPcQRUTIOEbY9Ocx1C00yn10LGGGdWyLKlXxGOtqqA1MeyJnqF6IeiJqzDsWM5RdxvYxtgTYNSWMfZU2FBsRQ2nCuvpZ0xgct7q6GXvFOBLzoWIeTK+NtRRM1jnJMSzljLXdjGeiWDIORHyB6lO8059xG+CM/ZmM+0qbQpGrbMKWSrHKjx03fLLqyR5KGt5HI6T7IQDkjXPyZxYIwT+ncQhEDcWgtVcc8T7KGp0IqzBOWgP6pCCEECJASUEIIUSAkoIQQogAJQUhhBABNReai0aHnGrVsiPwYwdWfrW/es4aWVjFtnLJmLd1PQfgJGAWmsPWWvHxzHIjHOK3JmJYHRi9ahCzrA7IGlr2FJYlSNRophMxzsmEA1bxtGTUyUJGYxZ2nILRrMSyyogYnigxs5EUOTYdyYuEAABjDWPkOmNWYdK4nrBxbGuW7JwzO1r4IYxz7usbovE8uRchY4+nrGZCxJoFsBs1sQY0zhKeGPGkIXhoSvF4Q9K3iukf5wX1fXm+yS2hCtv71p4YL5jSk7dEnxSEEEIEKCkIIYQIUFIQQggRoKQghBAiQElBCCFEQM3qo4IhB7EUDqwxiaU+cka13ehLQtUtzlLOWF12LIyvr7PZx4yvzFedodYxLB2YuiVijLVsEaz0btsokCYuVkMi4xhRQ2Vkql6Y+sg0o+AUjX1YIrYdBdKMCQBCxmIlE1xRYllxsCY2ReOc1mY21W7k2JaqK2Q2jOLHtmwXmCBvX/8oHRs1VGDhCH+lxA+g6UvMUBlFY/z6WfMZAKg6X4FjPZtV4+0UNlRjE4YtxkRhwosVjYY8psrIeMTTSX9tF0xrpmMHRnP8IDWgTwpCCCEClBSEEEIEKCkIIYQIUFIQQggRoKQghBAiIOQs4xQhhBD/7dAnBSGEEAFKCkIIIQKUFIQQQgQoKQghhAhQUhBCCBGgpCCEECJASUEIIUSAkoIQQogAJQUhhBAB/x8mvqRmdeltMwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b_AnJj5LZq8Y"
      },
      "execution_count": 113,
      "outputs": []
    }
  ]
}
